\documentclass[12pt]{article}

%%% JASA. Attention Mac Users. The latest version of the pdfTex compiler for Mac (pdfTeX-1.40.11) outputs files not currently supported by ScholarOne Manuscripts. To resolve this and have a PDF file that will convert properly, the PDF file will need to be distilled to Adobe version 1.4 or earlier. To create a PDF file that uploads correctly, place the command \pdfminorversion=4 in the preamble (before the begin{document}) of your LaTeX file and run pdfLaTeX again.

\pdfminorversion=4
 
<<debug,echo=F>>=
# run_level <- "jasa-sept-2020"
# run_level <- "manual"
 run_level <- 1

if(is.numeric(run_level)){
  bm_run_level <- run_level
  mscale_run_level <- run_level
  slice_run_level <- run_level
}

if(run_level=="manual"){
  bm_run_level <- 4
  mscale_run_level <- 5
  slice_run_level <- 3
}

if(run_level=="jasa-sept-2020"){  
#### values used for jasa submission, sept 2020  
  bm_run_level <- 4
  mscale_run_level <- 5
  slice_run_level <- 3
}

@

%%% IN THE FINAL VERSION, THE INPUT HEADER FILE SHOULD BE PASTED IN.
\input{theorems.tex}
\input{header-ms.tex}

\usepackage{amsmath,amssymb}
%\usepackage{graphicx,psfrag,epsf}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

\newcommand\rewrite[1]{{\textcolor{red}{#1}}}
%\newcommand\jpc[1]{{\textcolor{orange}{#1}}}
\newcommand\aak[1]{{\textcolor{blue}{#1}}}

\usepackage{fullpage}
% DON'T change margins - should be 1 inch all around.
%\addtolength{\oddsidemargin}{-.5in}%
%\addtolength{\evensidemargin}{-.5in}%
%\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
%%\addtolength{\textheight}{.3in}%
%\addtolength{\topmargin}{-.8in}%

\bibliographystyle{apalike}


% to run code from R: knitr::purl("ms.Rnw") ; source("ms.R")

<<packages,include=F,cache=F>>=
library("ggplot2")
library("spatPomp")
library(doParallel)
library(doRNG)

# 40 cores for doob, 8 for ito
doob_cores <- 40
gl_cores <- 36

cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset=NA))
if(is.na(cores)) cores <- detectCores()  
registerDoParallel(cores)

<<set-opts,include=F,cache=F>>=

options(
        scipen=2,
        help_type="html",
        stringsAsFactors=FALSE,
        prompt="R> ",
        continue="+  ",
        width=70,
        useFancyQuotes=FALSE,
        reindent.spaces=2,
        xtable.comment=FALSE
        )
@

<<knitr-opts,include=F,cache=F,purl=F>>=
library("knitr")
opts_knit$set(concordance=TRUE)
opts_chunk$set(
    progress=TRUE,prompt=TRUE,highlight=FALSE,
    tidy=TRUE,
    tidy.opts=list(
        keep.blank.line=FALSE
    ),
    comment="",
    warning=FALSE,
    message=FALSE,
    error=TRUE,
    echo=TRUE,
    cache=FALSE,
    strip.white=TRUE,
    results="markup",
    background="#FFFFFF00",
    size="normalsize",
    fig.path="figure/",
    fig.lp="fig:",
    fig.align="left",
    fig.show="asis",
#    dpi=300,
    dev="pdf",
    dev.args=list(
        bg="transparent",
        pointsize=9
    )
)

myround<- function (x, digits = 1) {
  # taken from the broman package
  if (digits < 1) 
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

@


% \date{This manuscript was compiled on \today}

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


\date{}

\if1\blind
{
  \title{\vspace{-10mm}\bf \mytitle}
  \author{Edward L. Ionides\thanks{This work was supported by National Science Foundation grants DMS-1761603 and DMS-1646108, and National Institutes of Health grants 1-U54-GM111274, 1-U01-GM110712 and 1-R01-AI143852. We acknowledge constructive feedback from two anonymous referees and the associate editor.}\hspace{.2cm}\\
    Department of Statistics, University of Michigan\\
    Kidus Asfaw\\
    Department of Statistics, University of Michigan\\
    %and \\
    Joonha Park\\
    Department of Mathematics, University of Kansas\\
    %and\\
    Aaron A. King\\
    Department of Ecology and Evolutionary Biology \& \\ Center for the Study of Complex Systems, University of Michigan
}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf \mytitle}
\end{center}
  \medskip
} \fi

\vspace{-10mm}

\begin{abstract}
Bagging (i.e., bootstrap aggregating) involves combining an ensemble of bootstrap estimators.
We consider bagging for inference from noisy or incomplete measurements on a collection of interacting stochastic dynamic systems.
Each system is called a unit, and each unit is associated with a spatial location.
A motivating example arises in epidemiology, where each unit is a city: the majority of transmission occurs within a city, with smaller yet epidemiologically important interactions arising from disease transmission between cities.
Monte~Carlo filtering methods used for inference on nonlinear non-Gaussian systems can suffer from a curse of dimensionality as the number of units increases.
We introduce bagged filter (BF) methodology which combines an ensemble of Monte Carlo filters, using spatiotemporally localized weights to select successful filters at each unit and time.
We obtain conditions under which likelihood evaluation using a BF algorithm can beat a curse of dimensionality, and we demonstrate applicability even when these conditions do not hold.
BF can out-perform an ensemble Kalman filter on a coupled population dynamics model describing infectious disease transmission.
A block particle filter also performs well on this task, though the bagged filter respects smoothness and conservation laws that a block particle filter can violate.

\end{abstract}

%%% JASA instructions
%% Should be written with the following elements in the following order: title page; author footnote; abstract; keywords; article text (table(s); figures); acknowledgments; appendices; references
%% Should be no more than 35 pages, inclusive of the abstract, tables, references, figure captions, footnotes, endnotes.
%% Should contain an unstructured abstract of 200 words.
%% Should contain between 3 and 5 keywords. Read making your article more discoverable, including information on choosing a title and search engine optimization.

\noindent%
{\it Keywords:}  Particle filter; Sequential Monte Carlo; Markov process; Population dynamics

\vspace{3mm}

\noindent This document is a reproducible version of \citet{ionides21} compiled on \today, using R version \Sexpr{getRversion()}, \code{pomp} version \Sexpr{packageVersion("pomp")} and \code{spatPomp} version \Sexpr{packageVersion("spatPomp")}.

%\vfill
\pagebreak

%\spacingset{1.45} % DON'T change the spacing!
\spacingset{1}

\section{Introduction}

Bagging is a technique to improve numerically unstable estimators by combining an ensemble of replicated bootstrap calculations \citep{breiman96}.
In the context of nonlinear partially observed dynamic systems, the bootstrap filter of \citet{gordon93} has led to a variety of particle filter (PF) methodologies \citep{doucet01,doucet11};
Here, we consider algorithms combining an ensemble of replicated particle filters, which we term {\it bagged filter} algorithms.
Standard PF methods suffer from a curse of dimensionality (COD), defined as an exponential increase in computational requirement as the problem size grows, limiting its applicability to large systems \citep{bengtsson08,snyder15,rebeschini15}.
The COD presents empirically as numerical instability of the Monte Carlo algorithm for affordable numbers of particles.
Much previous research has investigated scalable approaches to filtering and inference with applications to spatiotemporal systems.
Our bagged filters are in the class of {\it plug-and-play} algorithms, meaning that they require as input a simulator for the latent dynamic process but not an evaluator of transition probabilities \citep{breto09,he10}.
Similar properties to plug-and-play are {\it likelihood-free} \citep{brehmer20} and {\it equation-free} \citep{kevrekidis09}.
The ensemble Kalman filter \citep{evensen09book,lei10,katzfuss19} is a widely used plug-and-play method which uses simulations to construct a nonlinear filter that is exact for a linear Gaussian model.
Another plug-and-play approach to combat the COD is the block particle filter \citep{rebeschini15,ng02}.
Both ensemble Kalman filter and block particle filter methods construct trajectories that can violate smoothness and conservation properties of the dynamic model.
By contrast, our bagged filters are built using valid trajectories of the dynamic model, making localization approximations only when comparing these trajectories to data.

The replicated stochastic trajectories in a bagged filter form an ensemble of representations of the dynamic system.
Unlike the particles in a particle filter or ensemble Kalman filter, the bagged replicates are independent in a Monte Carlo sense.
Bagged filters therefore bear some resemblance to \textit{poor man's ensemble} forecasting methodology in which a collection of independently constructed forecasts is generated using different models and methods \citep{ebert01}. 
Poor man's ensembles have sometimes been found to have greater forecasting skill than any one forecast \citep{leutbecher08,palmer02,chandler13}.
One explanation for this phenomenon is that even a hypothetically perfect model cannot provide effective filtering using methodology afflicted by the COD.  
We show that bagged filter methodology can relieve this limitation.
From this perspective, the independence of the forecasts in the poor man's ensemble, rather than the diversity of model structures, may be the key to its success.


We first consider a simple bagged filter where each replicate is an independent simulation of the latent process model.
We call this the unadapted bagged filter ({\UBF}) since the replicates in the ensemble depend on the model but not on the data.
{\UBF} is described in Sec.~\ref{sec:ubf}, with a theoretical analysis presented in Sec.~\ref{sec:ubf:theory}.
Each {\UBF} replicate corresponds to a basic PF algorithm with a single particle.
We show that {\UBF} formally beats the COD under a weak mixing assumption, though {\UBF} can have poor numerical behavior if a very large number of replicates are needed to reach this happy asymptotic limit.
Subsequent empirical results show that {\UBF} may nevertheless be a useful algorithm in some situations.
In Sec.~\ref{sec:abf}, we generalize {\UBF} to construct an adapted bagged filter (\ABF) where each replicate tracks the data.
The price of adaptation is that {\ABF} no longer avoids the COD, a limitation that can be controlled in certain situations by supplementing {\ABF} with a technique called intermediate resampling, to obtain the {\ABFIR} algorithm.
Theoretical results for {\ABF} and {\ABFIR} algorithms are developed in Sec.~\ref{sec:abf:theory}.
The algorithms are demonstrated in action and compared with alternative approaches in Sec.~\ref{sec:examples}.

\section{The unadapted bagged filter (UBF)}
\label{sec:ubf}

Suppose the collection of units is indexed by the set $\{1,2,\dots,\Unit\}$, which is written as $1\mycolon\Unit$.
The latent Markov process is denoted by $\{\myvec{X}_{\time},\time\in 0\mycolon\Time\}$, with $\myvec{X}_{\time}=X_{1:\Unit,\time}$ taking values in a product space $\Xspace^\Unit$.
This discrete time process may arise from a continuous time Markov process $\{\myvec{X}(t), t_0\le t\le t_{\Time} \}$ observed at times $t_{1:\Time}$, and in this case we set $\myvec{X}_\time=\myvec{X}(t_\time)$.
The initial value $\myvec{X}_{0}$ may be stochastic or deterministic.
Observations are made on each unit, modeled by an observable process $\{\myvec{Y}_{\time}=Y_{1:\Unit,\time},\time\in 1\mycolon\Time\}$ which takes values in a product space $\Yspace^\Unit$.
Observations are modeled as being conditionally independent given the latent process.
The conditional independence of measurements applies over both time and the unit structure, so the collection $\big\{Y_{\unit\comma\time},\unit\in\seq{1}{\Unit},\time\in\seq{1}{\Time}\big\}$ is conditionally independent given
$\big\{X_{\unit\comma\time},\unit\in\seq{1}{\Unit},\time\in\seq{1}{\Time}\big\}$.
The unit structure for the observation process is not necessary for all that follows (see Sec.~\SuppSecGeneralization).
We suppose the existence of a joint density $f_{\myvec{X}_{0:\Time},\myvec{Y}_{1:\Time}}$  of $X_{1:\Unit,0:\Time}$ and $Y_{1:\Unit,1:\Time}$ with respect to some appropriate measure, following a notational convention that the subscripts of $f$ denote the joint or conditional density under consideration.
The data are $\data{y}_{\unit\comma\time}$ for unit $\unit$ at time $\time$.
This model is a special case of a  partially observed Markov process  \citep[POMP,][]{breto09}, also known as a state space model or hidden Markov model.
The additional unit structure, not generally required for a POMP, is appropriate for modeling interactions between units characterized by a spatial location, and so we call the model a SpatPOMP.
In the following, we use a lexicographical ordering on the set of observations;
Specifically, we define the set of observations preceding unit $\unit$ at time $\time$ as
\begin{equation}\label{eq:setA}
A_{\unit\comma\time}=\big\{(\tilde\unit,\tilde\time): 1 \leq \tilde\time<\time \mbox{ or } (\tilde\time=\time \mbox{ and } \tilde\unit <\unit)\big\}.
\end{equation}
The ordering of the spatial locations in \myeqref{eq:setA} might seem artificial, and indeed densities such as
$f_{X_{\unit\comma\time}|X_{A_{\unit\comma\time}}}$ will frequently be hard to compute or simulate from.
The bagged filter algorithms we study do not evaluate or simulate such transition densities but only compute the measurement model on neighborhoods, unlike the filter of \citet{beskos17} built on a similar factorization. 
If sufficiently distant units are approximately independent, we say the system is {\it weakly coupled}.
In this case, we suppose there is a neighborhood $B_{\unit\comma\time}\subset A_{\unit\comma\time}$ such that the latent process on 
$A_{\unit\comma\time} \setminus B_{\unit\comma\time}$ 
is approximately conditionally independent of $X_{\unit\comma\time}$ given data on $B_{\unit\comma\time}$. 

Our primary interest is estimation of the log likelihood for the data given the model, $\loglik=\log f_{\myvec{Y}_{1:\Time}}(\data{\myvec{y}}_{1:\Time})$, which is of fundamental importance in both Bayesian and non-Bayesian statistical inference.
A general filtering problem is to evaluate $\E\big[h(X_{\unit,\time}) \given Y_{A_{\unit,\time}}\equals \data{y}_{A_{\unit,\time}}\big]$ for some function $h:\Xspace\to\R$.
Taking $h(x)= f_{Y_{\unit,\time}| X_{\unit,\time}}\big(\data{y}_{\unit,\time}\given x\big)$ gives a filtering representation of the likelihood evaluation problem.
Further discussion on bagged filtering for other filtering problems is given in Sec.~{\SuppSecLatentStates}.
For likelihood-based inference, maximization plays an important role in point estimation, confidence interval construction, hypothesis testing and model selection. 
An extension of bagged filtering to likelihood maximization is demonstrated in Sec.~\ref{sec:profile} following the approach described in Sec.~{\SuppSecParameterEst}.

Pseudocode for a {\UBF} algorithm for likelihood evaluation is given below.
The prediction weight  $w^P_{u,n,i}$ gives an appropriate weighting for replicate $i$ for predicting $\data{y}_{u,n}$ based on the most relevant data, $\data{y}_{B_{u,n}}$.
Conditional log likelihoods are estimated using an approximation
\begin{eqnarray*}
    \ell_{u,n} &=& \log f_{Y_{u,n}|Y_{A_{u,n}}}\big(\data{y}_{u,n}\given \data{y}_{A_{u,n}}\big)
    = \log \left( \int f_{Y_{u,n}|X_{u,n}}(\data{y}_{u,n}\given x) \, f_{X_{u,n}|Y_{A_{u,n}}}(x \given \data{y}_{A_{u,n}}) \, dx \right)
\\
  &\approx& \log \left( \int f_{Y_{u,n}|X_{u,n}}(\data{y}_{u,n}\given x) \, f_{X_{u,n}|Y_{B_{u,n}}}(x\given \data{y}_{B_{u,n}}) \, dx \right).
\end{eqnarray*}
The choice of $B_{u,n}$ is determined empirically, with a bias-variance trade-off used to compare small neighborhoods such as $B_{u,n} = \{ (u,n-1),(u-1,n)\}$ or $B_{u,n} = \{ (u,n-1),(u,n-2)\}$ against larger neighborhoods.
The plug-and-play property is evident because {\UBF} requires as input a simulator for the latent coupled dynamic process but not an evaluator of transition probabilities.

%%%%%%%%%%%%% uuuuuuuuu

% UBF PSEUDOCODE uuuuuuuuuu
\begin{algorithm}[H]
  \caption{\bf Unadapted bagged filter (UBF).
   }\label{alg:ubf}
  \KwIn{
    simulator for $f_{\myvec{X}_{\time}|\myvec{X}_{\time-1}}(\myvec{x}_{\time}\given \myvec{x}_{\time-1})$ and $f_{\myvec{X}_0}(\myvec{x}_0)$;
    evaluator for $f_{{Y}_{\unit,\time}|{X}_{\unit,\time}}({y}_{\unit,\time}\given {x}_{\unit,\time})$;
    data, $\data{\myvec{y}}_{1:\Time}$;
    number of replicates, $\Rep$;
    neighborhood structure, $B_{\unit,\time}$
  }
\For{$\rep\ \mathrm{in}\ \seq{1}{\Rep}$}{
initialize simulation, $\myvec{X}_{0,\rep} \sim f_{\myvec{X}_0}(\cdot)$
  \;
  \nllabel{alg:ubf:for:n}
\For{$\time\ \mathrm{in}\ \seq{1}{\Time}$}{
simulate,
    $\myvec{X}_{\time,\rep} \sim
      f_{\myvec{X}_{\time}|\myvec{X}_{\time-1}}
      \big( \mydot \given \myvec{X}_{\time-1,\rep}
    \big)$
  \nllabel{alg:ubf:adapted:proposals}
  \;
measurement weights,
  $w^M_{\unit,\tilde n,\rep}=
    f_{Y_{\unit,\time}|X_{\unit,\time}}
    \big (\data{y}_{\unit,\time}\given {X}_{\unit,\time,\rep}
  \big)$
    for $\unit$ in $\seq{1}{\Unit}$
  \;
prediction weights,
  $w^P_{\unit,\time,\rep}=\prod_{(\tilde \unit,\tilde n)\in B_{\unit,\time}} w^M_{\tilde\unit,\tilde n,\rep}$
    for $\unit$ in $\seq{1}{\Unit}$
      \nllabel{alg:ubf:adapted:weights}
  \;
  }
}
%conditional log likelihood,
  $\MC{\loglik}_{\unit,\time}= 
    \log\left(
      \sum_{\rep=1}^\Rep w^M_{\unit,\time,\rep}w^P_{\unit,\time,\rep}
    \right)
    -\log\left(
      \sum_{\rep=1}^\Rep w^P_{\unit,\time,\rep}
    \right)$
  for $\unit$ in $\seq{1}{\Unit}$, $\time$ in $\seq{1}{\Time}$
  \;
\KwOut{
log likelihood estimate, $\MC{\loglik}= \sum_{\time=1}^\Time\sum_{\unit=1}^\Unit \MC{\loglik}_{\unit\comma\time}$\\
}
\end{algorithm}

\subsection{{\UBF} theory}
\label{sec:ubf:theory}

A dataset $\data{\myvec{y}}_{1:\Time}$ with $\Unit$ units is modeled via a joint density  $f_{\myvec{X}_{0:\Time},\myvec{Y}_{1:\Time}}$.
We consider non-asymptotic bounds that apply for all values of $\Unit$ and $\Time$. 
To impose a requirement that distant regions of space-time behave similarly and have only weak dependence, we assert the following conditions which define constants $\eone$, $\etwo$ and $Q$ used to bound the bias and variance in Theorem~\ref{thm:tif}.
Stronger bounds are obtained when the conditions hold for small $\eone$, $\etwo$ and $Q$.

\Ai  %% Assumption 1  Assumption~\ref{A1}

\Aii  %% Assumption 2  Assumption~\ref{A1b}

\Aiii   %% Assumption 3  Assumption~\ref{A2}

\Aiv    %% Assumption 4  Assumption~\ref{A:unconditional:mix}


The two mixing conditions in Assumptions~\ref{A1} and~\ref{A:unconditional:mix} are subtly different. 
Assumption~\ref{A1} describes a conditional mixing property dependent on the data, whereas~\ref{A:unconditional:mix} asserts a form of unconditional mixing.
Although both capture a similar concept of weak coupling, conditional and unconditional mixing properties do not readily imply one another.
Assumption~\ref{A2} is a compactness condition of a type that has proved useful in the theory of particle filters despite the rarity of its holding exactly.
Theorem~\ref{thm:tif} shows that these conditions let {\UBF} compute the likelihood with a Monte Carlo variance of order $\Unit\Time\Rep^{-1}$ with a bias of order $\Unit\Time\epsilon$.

\TheoremI %% Theorem~\ref{thm:tif}

\begin{proof}
A complete proof is given in Sec.~\SuppSecThmI. 
Briefly, the assumptions imply a multivariate central limit theorem for $\{\MC{\loglik}_{\unit\comma\time}, (\unit,\time)\in \UnitSet{\times}\ObsTimeSet\}$ as $\Rep\to\infty$.
The limiting variances and covariances are uniformly bounded, using Assumptions~\ref{A1b} and~\ref{A2}. 
Assumption~\ref{A1} provides a uniform bound on the discrepancy between ${\loglik}_{\unit\comma\time}$ and mean of the Gaussian limit.
This is enough to derive \myeqref{th1:lik:bound}.
Assumption~\ref{A:unconditional:mix} gives a stronger bound on covariances between sufficiently distant units, leading to \myeqref{th1:lik:bound2}.
\end{proof}

Theorem~\ref{thm:tif} does not guarantee uniformity over $\Unit$ and $\Time$ of the rate of convergence as $\Rep\to\infty$.
However, it does guarantee that the polynomial bounds in (\ref{th1:lik:bound}) and (\ref{th1:lik:bound2}) hold for sufficiently large $\Rep$.
The COD is characterized by exponential bounds, and so Theorem~\ref{thm:tif} shows a specific sense in which {\UBF} can avoid COD.
%% This formalizes the heuristic that {\UBF} uses prediction weights $w^P_{u,n,i}$ which scale exponentially with the size of local neighborhoods $B_{u,n}$, avoiding the use of weights that scale exponentially with increasing $U$.
Uniformity of the central limit convergence in Theorem~\ref{thm:tif} may be expected to hold via a Berry-Esseen theorem, but extension of existing Berry-Esseen results for dependent processes \citep{bentkus97,jirak16} is beyond the scope of this article.

The approximation error for {\UBF} can be divided into two sources: a localization bias due to conditioning on a finite neighborhood, and Monte Carlo error. 
The localization bias does not disappear in the limit as Monte Carlo effort increases.
It does become small as the conditioning neighborhood increases, but the Monte Carlo effort grows exponentially in the size of this neighborhood.
Although the filtering inference is carried out using localization, the simulation of the process is carried out globally which avoids the introduction of additional boundary effects and ensures that the simulations comply with any constraints satisfied by the model for the latent process.

\section{Adaptation and intermediate resampling}
\label{sec:abf}


Theorem~\ref{thm:tif} shows that {\UBF} can beat COD.
However, {\UBF} can perform poorly on long time series unless weak temporal dependence allows simulated sample paths to remain relevant over the course of a long time series. 
For example, we will find that {\UBF} performs well on an epidemiological model (Sec.~\ref{sec:discussion}) but less well on a geophysical model (Sec.~{\SuppSecLorenz}).
It is sometimes necessary to select simulations consistent with the data, much as standard PF algorithms do.
We look for approaches that build on the basic insight of {\UBF} while having superior practical performance.


Whereas the full global filtering problem of drawing from $f_{\myvec{X}_{\time}|\myvec{Y}_{1:\time}}$ may be intractable via importance sampling methods, a version of this problem localized in space and time may nevertheless be feasible.
The conditional density, $f_{\myvec{X}_{\time}|\myvec{Y}_{\time},\myvec{X}_{\time-1}}$, is called the {\it adapted density}, and simulating from this density is called {\it adapted simulation}.
For models where $\myvec{X}_{\time-1}$ is highly informative about $\myvec{X}_{\time}$, importance sampling for adapted simulation may be much easier than the full filter calculation.
The following adapted bagged filter ({\ABF}) is constructed under a hypothesis that the adapted simulation problem is tractable, and it is applicable when the number of units is prohibitive for Monte Carlo sampling from the full filter distribution but not for sampling from the adapted distribution.
In {\ABF}, the adapted simulations are reweighted in a neighborhood of each unit and time point to construct a local approximation to the filtering problem which leads to an estimate of the likelihood.
The pseudocode for {\ABF}, below, reduces to {\UBF} when using a single particle per repicate, $\Np=1$.

%%%%%%%%%% aaaaaaaaaaaaaaaaa


% ABF PSEUDOCODE AAAAAAAAAAAAAAAAAAAA
\begin{algorithm}[H]
  \caption{\bf Adapted bagged filter (ABF)
  }\label{alg:abf}
  \KwIn{
    simulator for $f_{\myvec{X}_{\time}|\myvec{X}_{\time-1}}(\myvec{x}_{\time}\given \myvec{x}_{\time-1})$ and $f_{\myvec{X}_0}(\myvec{x}_0)$;
    evaluator for $f_{{Y}_{\unit,\time}|{X}_{\unit,\time}}({y}_{\unit,\time}\given {x}_{\unit,\time})$;
    data, $\data{\myvec{y}}_{1:\Time}$;
    number of particles per replicate, $\Np$;
    number of replicates, $\Rep$;
    neighborhood structure, $B_{\unit,\time}$
  }
\For{$\rep\ \mathrm{in}\ \seq{1}{\Rep}$}{
initialize adapted simulation, $\myvec{X}^{\IF}_{0,\rep} \sim f_{\myvec{X}_0}(\cdot)$
  \;
  \nllabel{alg:abf:for:n}
\For{$\time\ \mathrm{in}\ \seq{1}{\Time}$}{
proposals:
  $\myvec{X}_{\time,\rep,\np}^{\IP} \sim 
    f_{\myvec{X}_{\time}|X_{1:\Unit,\time-1}} 
    \big( \myvec{x}_{\time}\given \myvec{X}^{\IF}_{\time-1,\rep}\big)$
  for $\np$ in $\seq{1}{\Np}$
  \nllabel{alg:abf:adapted:proposals}
  \;
% measurement weights:
  $w^M_{\unit,\time,\rep,\np} = 
    f_{Y_{\unit,\time}|X_{\unit\comma\time}} 
    \big (\data{y}_{\unit\comma\time}\given X^{\IP}_{\unit\comma\time,\rep,\np}\big)$
  for $\unit$ in $\seq{1}{\Unit}$, $\np$ in $\seq{1}{\Np}$
  \;
adapted resampling weights,
    $w^{\IF}_{\time,\rep,\np} = 
    \prod_{\unit=1}^{\Unit} w^M_{\unit,\time,\rep,\np}$
    for $\unit$ in $\seq{1}{\Unit}$, 
    $\np$ in $\seq{1}{\Np}$
  \nllabel{alg:abf:adapted:weights}
  \;
%resampling:
  $\myvec{X}^{\IF}_{\time,\rep} = \myvec{X}^{\IP}_{\time,\rep,r(\rep)}$ with $\prob\big[\resampleIndex({\rep})=a \big] = w^{\IF}_{\time,\rep,a}
    \Big( 
    \sum_{\altNp=1}^{\Np} w^{\IF}_{\time,\rep,\altNp}
    \Big)^{-1}$ 
  \;
% Prediction weights:
  $w^{\LCP}_{\unit,\time,\rep,\np}= \displaystyle
  \prod_{\altTime=1}^{\time-1}
  \Big[
    \frac{1}{\Np}\sum_{k=1}^{\Np}
    \hspace{1mm}
       \prod_{(\altUnit,\altTime)\in B_{\unit,\time}} 
    \hspace{-3mm}
        w^M_{\altUnit,\altTime,\rep,k}
  \Big]
  \prod_{(\altUnit,\time)\in B_{\unit,\time}} 
    \hspace{-3mm}
        w^M_{\altUnit,\time,\rep,\np}$
      for $\unit$ in $\seq{1}{\Unit}$, $\np$ in $\seq{1}{\Np}$
  \nllabel{alg:abf:lp:weights} \nllabel{alg:abf:end:n}
  \;
}
}
  $ \displaystyle
    \MC{\loglik}_{\unit,\time}= 
    \log\Bigg(
      \frac{
      \sum_{\rep=1}^\Rep \sum_{\np=1}^{\Np} w^M_{\unit,\time,\rep,\np}w^P_{\unit,\time,\rep,\np}
      }{
      \sum_{\rep=1}^\Rep \sum_{\np=1}^{\Np} w^P_{\unit,\time,\rep,\np}
      }
   \Bigg)$
   for $\unit$ in $\seq{1}{\Unit}$, $\time$ in $\seq{1}{\Time}$
   \;
\KwOut{
log likelihood estimate, $\MC{\loglik}= \sum_{\time=1}^\Time\sum_{\unit=1}^\Unit \MC{\loglik}_{\unit\comma\time}$\\
}
\end{algorithm}

{\ABF} remedies a weakness of {\UBF} by making each boostrap filter adapted to the data.
However, this benefit carries a cost, since adapted simulation is not immune from the curse of dimensionality.
Therefore, we also consider an algorithm called {\ABFIR} which uses an intermediate resampling technique to carry out the adapted simulation.
Intermediate resampling involves assessing the satisfactory progress of particles toward the subsequent observation at a collection of times between observations.
This is well defined when the latent process has a continuous time representation, $\{\myvec{X}(t)\}$, with observation times $t_{1:\Time}$.
We write $\Ninter$ intermediate resampling times as 
\begin{equation}
\nonumber
t_{\time-1}=t_{\time,0}<t_{\time,1}<\dots<t_{\time,\Ninter}=t_{\time}.
\end{equation}
Carrying out an intermediate resampling procedure can have favorable scaling properties when $\Ninter$ is proportional to $\Unit$ \citep{park20}.
In the case $\Ninter=1$, {\ABFIR} reduces to {\ABF}.
Intermediate resampling was developed in the context of sequential Monte Carlo \citep{delmoral15,park20}; however, the same theory and methodology can be applied to the simpler and easier problem of adapted simulation. 
{\ABFIR} employs a guide function to gauge the compatibility of each particle with future data.
This is a generalization of the popular auxiliary particle filter \citep{pitt99}.
Only an ideal guide function fully addresses COD \citep{park20} and on nontrivial problems this is not available.
However, practical guide functions can nevertheless improve performance.

The implementation in the {\ABFIR} pseudocode constructs the guide $g_{\time,\ninter,\rep,\np}$ using a simulated moment method proposed by \citet{park20}.
The quantities  $\myvec{X}_{\time,\rep,\npgir}^{G}$, $V_{\unit,\time,\rep}$, $\myvec{\mu}^{\GP}_{\time,\ninter,\rep,\np}$, $V^{\mathrm{meas}}_{\unit,\time,\ninter,\rep,\np}$, $V^{\mathrm{proc}}_{\unit,\time,\ninter,\rep}$ and $\theta_{\unit,\time,\ninter,\rep,\np}$ constructed in {\ABFIR} are used only to construct $g_{\time,\ninter,\rep,\np}$.
Heuristically, we use guide simulations to approximate the variance of the increment in each particle between time points, and we augment the measurement variance to account for both dynamic variability and measurement error.
The guide function affects numerical performance of the algorithm but not its correctness: it enables a computationally convenient approximation to improve performance on the intractable target problem.
Our guide function supposes the availability of a deterministic function approximating evolution of the mean of the latent process, written as
\begin{equation}
\nonumber
\myvec{\mu}(\myvec{x},s,t)\approx \E\big[\myvec{X}(t) \given \myvec{X}(s)=\myvec{x}\big]. 
\end{equation}
Further, the guide requires that the measurement model has known conditional mean and variance as a function of the model parameter vector $\theta$, written as
\begin{eqnarray}
\nonumber
h_{\unit\comma\time}(x_{\unit\comma\time})
  &=& \E\big[Y_{\unit\comma\time}\given X_{\unit\comma\time}=x_{\unit\comma\time}\big]
\\
\nonumber
{\thetaToV}_{\unit\comma\time}(x_{\unit\comma\time},\theta)
  &=& \var\big(Y_{\unit\comma\time}\given X_{\unit\comma\time}=x_{\unit\comma\time} \giventh \theta\big)
\end{eqnarray}
Also required for {\ABFIR} is an inverse function ${\VtoTheta}_{\unit\comma\time}\big(V,x_{\unit\comma\time},\theta\big)$ such that ${\VtoTheta}_{\unit\comma\time}\big(V,x_{\unit\comma\time},\theta\big)=\phi$ implies
\begin{equation}
\nonumber
{\thetaToV}_{\unit\comma\time} \big( x_{\unit\comma\time},\phi \big) = V.
\end{equation}

%%%%%%%%%%%%%%%%% iiiiiiiiiiiiiiiiiii

% ABF-IR PSEUDOCODE iiiiiiiiii
\begin{algorithm}[H]
  \caption{\bf ABF with intermediate resampling (ABF-IR)
  }\label{alg:abfir}
  \KwIn{same as Algorithm~\ref{alg:abf} (ABF) plus:
%    simulator for $f_{\myvec{X}_{\time}|\myvec{X}_{\time-1}}(\myvec{x}_{\time}\given \myvec{x}_{\time-1})$ and $f_{\myvec{X}_0}(\myvec{x}_0)$;
%    evaluator for $f_{{Y}_{\unit,\time}|{X}_{\unit,\time}}({y}_{\unit,\time}\given {x}_{\unit,\time})$;
%    data, $\data{\myvec{y}}_{1:\Time}$;
%    number of particles per replicate, $\Np$;
%    number of replicates, $\Rep$;
%    neighborhood structure, $B_{\unit,\time}$;
    intermediate timesteps, $\Ninter$;
    measurement variance functions, ${\VtoTheta}_{\unit\comma\time}$ and ${\thetaToV}_{\unit\comma\time}$;
    approximate mean functions, $\myvec{\mu}$ and $h_{\unit\comma\time}$
  }
\For{$\rep\ \mathrm{in}\ \seq{1}{\Rep}$}{  
initialize adapted simulation, $\myvec{X}^{\IF}_{0,\rep} \sim f_{\myvec{X}_0}(\cdot)$
  \;
  \nllabel{alg:abfir:for:n}  
\For{$\time\ \mathrm{in}\ \seq{1}{\Time}$}{
  guide simulations:
    $\myvec{X}_{\time,\rep,\npgir}^{G} \sim 
    f_{\myvec{X}_{\time}|\myvec{X}_{\time-1}} 
    \big( \myvec{x}_{\time}\given \myvec{X}^{\IF}_{\time-1,\rep} \big)$
  for $\np$ in $\seq{1}{\Np}$
  \;
% Guide sample variance:
  $V_{\unit,\time,\rep}=
    \var \big\{
    h_{\unit\comma\time}\big( {X}_{\unit,\time,\rep,\npgir}^{G}\big), \ 
      \npgir \mbox{ in } \seq{1}{\Npgir}
      \big\}$
  \;
  $\guideFunc^{\resample}_{\time,0,\rep,\np}=1 \; \; $ and
    $\; \myvec{X}_{\time,0,\rep,\np}^{\GR}=\myvec{X}^{\IF}_{\time-1,\rep}$
  for $\np$ in $\seq{1}{\Np}$
  \;
%%% intermediate loop sssss  
  \For{$\ninter\ \mathrm{in}\ \seq{1}{\Ninter}$}{
%Intermediate proposals:
     ${\myvec{X}}_{\time,\ninter,\rep,\np}^{\GP}
       \sim {f}_{{\myvec{X}}_{\time,\ninter}|{\myvec{X}}_{\time,\ninter-1}}
       \big(\mydot|{\myvec{X}}_{\time,\ninter-1,\rep,\np}^{\GR}\big)$
     for $\np$ in $\seq{1}{\Np}$
     \;
     $\myvec{\mu}^{\GP}_{\time,\ninter,\rep,\np} 
       = \myvec{\mu}\big( \myvec{X}^{\GP}_{\time,\ninter,\rep,\np},
       t_{\time,\ninter},t_{\time} \big)$
     for $\np$ in $\seq{1}{\Np}$
     \;
%Measurement variance at skeleton: 
     $V^{\mathrm{meas}}_{\unit,\time,\ninter,\rep,\np}
        = \thetaToV_{\unit}(\theta,\mu^{\GP}_{\unit,\time,\ninter,\rep,\np})$
     for $\unit$ in $\seq{1}{\Unit}$, $\np$ in $\seq{1}{\Np}$
     \;
%Process variance:
     $V^{\mathrm{proc}}_{\unit,\time,\ninter,\rep}
       = V_{\unit,\time,\rep} \,
       \big(t_{\time}-t_{\time,\ninter}\big) \Big/
       \big(t_{\time}-t_{\time,0}\big)$ 
     for $\unit$ in $\seq{1}{\Unit}$
     \;
%Moment matching:
     $\theta_{\unit,\time,\ninter,\rep,\np}= 
       \VtoTheta_{\unit}\big(
       V^{\mathrm{meas}}_{\unit,\time,\ninter,\rep,\np} +
       V^{\mathrm{proc}}_{\unit,\time,\ninter,\rep}, 
       \, \mu^{\GP}_{\unit,\time,\ninter,\rep,\np}
       \big)$
     for $\unit$ in $\seq{1}{\Unit}$, $\np$ in $\seq{1}{\Np}$
     \;
% Guide function: 
     $\guideFunc_{\time,\ninter,\rep,\np}=
       \prod_{\unit=1}^{\Unit}
       f_{Y_{\unit,\time}|X_{\unit,\time}}
       \big(
         \data{y}_{\unit,\time}\given \mu^{\GP}_{\unit,\time,\ninter,\rep,\np}
	 \giventh \theta_{\unit,\time,\ninter,\rep,\np} 
         \big)$
     for $\np$ in $\seq{1}{\Np}$
     \;
guide weights:
     $w^G_{\time,\ninter,\rep,\np}= \guideFunc^{}_{\time,\ninter,\rep,\np}
       \big/ \guideFunc^{\resample}_{\time,\ninter-1,\rep,\np}$     
     for $\np$ in $\seq{1}{\Np}$
     \;
resampling:
     $\prob\big[\resampleIndex({\rep,\np})=a \big] =
       w^G_{\time,\ninter,\rep,a}
       \Big( \sum_{\altNp=1}^{\Np}w^G_{\time,\ninter,\rep,\altNp}\Big)^{-1}$
     for $\np$ in $\seq{1}{\Np}$       
     \;
     $\myvec{X}_{\time,\ninter,\rep,\np}^{\GR}=
       \myvec{X}_{\time,\ninter,\rep,\resampleIndex({\rep,\np})}^{\GP}\; \; $
     and
     $\; \guideFunc^{\resample}_{\time,\ninter,\rep,\np}=
       \guideFunc^{}_{\time,\ninter,\rep,\resampleIndex({\rep,\np})}\,$
     for $\np$ in $\seq{1}{\Np}$       
  } 
  $\myvec{X}^{\IF}_{\time,\rep}=\myvec{X}^{\GR}_{\time,\Ninter,\rep,1}$
  \;
%  Measurement weights:
  $w^M_{\unit,\time,\rep,\npgir} = 
    f_{Y_{\unit,\time}|X_{\unit,\time}} 
    \big (\data{y}_{\unit,\time}\given X^{G}_{\unit,\time,\rep,\npgir} \big)$
  for $\unit$ in $\seq{1}{\Unit}$, $\np$ in $\seq{1}{\Np}$
  \;
% Prediction weights:
  $w^{\LCP}_{\unit,\time,\rep,\npgir}= \displaystyle
  \prod_{\altTime=1}^{\time-1}
  \Big[
    \frac{1}{\Npgir}\sum_{a=1}^{\Npgir}
    \hspace{1mm}
       \prod_{\altUnit:(\altUnit,\altTime)\in B_{\unit,\time}} 
    \hspace{-1mm}
        w^M_{\altUnit,\altTime,\rep,a}
  \Big] \prod_{\altUnit:(\altUnit,\time)\in B_{\unit,\time}} 
    \hspace{-1mm}
        w^M_{\altUnit,\time,\rep,\npgir}$
  for $\unit$ in $\seq{1}{\Unit}$, $\np$ in $\seq{1}{\Np}$      	
  \;
}
}
\KwOut{
$\displaystyle \MC{\loglik}=
\sum_{\unit=1}^{\Unit} \sum_{\time=1}^{\Time}
\log\Bigg(
\frac{
\sum_{\rep=1}^\Rep \sum_{\npgir=1}^{\Npgir} w^M_{\unit,\time,\rep,\npgir}w^P_{\unit,\time,\rep,\npgir}
}{
\sum_{\rep=1}^\Rep \sum_{\npgir=1}^{\Npgir} w^P_{\unit,\time,\rep,\npgir}
}
\Bigg) 
$
%for $\unit$ in $\seq{1}{\Unit}$, $\time$ in $\seq{1}{\Time}$
}
\end{algorithm}


This guide function is applicable to spatiotemporal versions of a broad range of population and compartment models used to model dynamic systems in ecology, epidemiology, and elsewhere. 
Other guide functions could be developed and inserted into the {\ABFIR} algorithm, including other constructions considered by \citet{park20}.

One might wonder why it is appropriate to keep many particle representations at intermediate timesteps while resampling down to a single representative at each observation time.
An answer is that adaptive simulation can fail to track the observation sequence when one resamples down to a single particle too often (Sec.~\SuppSecAdaptedSimulation).

%% llllllllllllllllllllllllllllllllll

%\section{Likelihood factorizations and their approximations}

\subsection{{\ABFIR} theory}
\label{sec:abf:theory}


We  start by considering a deterministic limit for infinite Monte Carlo effort and explaining why the {\ABF} and {\ABFIR} algorithms approximately target the likelihood function, subject to suitable mixing behavior.
Subsequently, we consider the scaling properties as Monte Carlo effort increases.
We adopt a convention that densities involving $Y_{\unit\comma\time}$ are implicitly evaluated at the data, $\data{y}_{\unit\comma\time}$, and densities involving $X_{\unit\comma\time}$ are implicitly evaluated at $x_{\unit\comma\time}$ unless otherwise specified.
We write $A^{+}_{\unit\comma\time}=A_{\unit\comma\time}\cup (\unit,\time)$, matching the defintion $B^{+}_{\unit\comma\time}=B_{\unit\comma\time}\cup (\unit,\time)$.
The essential ingredient in all the algorithms is a localization of the likelihood, which may be factorized sequentially as
\begin{equation}
%\label{eq:ordering}
\nonumber
f_{Y_{1:\Unit,1:\Time}}
=
\prod_{\time=1}^\Time\prod_{\unit=1}^\Unit 
f_{Y_{\unit\comma\time}|Y_{A_{\unit\comma\time}}}
= \prod_{\time=1}^\Time\prod_{\unit=1}^\Unit 
\frac{f_{Y^{}_{A^+_{\unit\comma\time}}}}{f_{Y^{}_{A^{}_{\unit\comma\time}}}}.
\end{equation}
In particular, the approximations assume that the full history $A_{\unit\comma\time}$ can be well approximated by a neighborhood $B_{\unit\comma\time}\subset A_{\unit\comma\time}$.
{\UBF} approximates $f_{Y_{\unit\comma\time}|Y_{A_{\unit\comma\time}}}$ by
\begin{equation}
\nonumber
f_{Y_{\unit\comma\time}|Y_{B_{\unit\comma\time}}} 
=
\frac{f^{}_{Y^{}_{B^+_{\unit\comma\time}}}}{f^{}_{Y^{}_{B^{}_{\unit\comma\time}}}}
=
\frac{\int f_{Y_{B^+_{\unit\comma\time}}|X_{B^+_{\unit\comma\time}}} f_{X_{B^+_{\unit\comma\time}}}\, dx_{B^+_{\unit\comma\time}} }
{\int f_{Y_{B_{\unit\comma\time}}|X_{B_{\unit\comma\time}}} f_{X_{B_{\unit\comma\time}}}\, dx_{B_{\unit\comma\time}} }.
\end{equation}
For $\unitTimeSubset \subset \seq{1}{\Unit}\times\seq{1}{\Time}$, define $\unitTimeSubset^{[m]}=\unitTimeSubset \cap \big(\UnitSet\times \{m\}\big)$.
{\ABF} and {\ABFIR} build on the following identity, 
\begin{equation}
\nonumber
%\label{eq:adapted}
\hspace{-1mm}
f_{Y_{A_{\unit\comma\time}}}{=} \int 
\! \!
f_{\myvec{X}_0} 
\! \!
\left[
\prod_{\altAltTime=1}^{\time}
f_{\myvec{X}_{\altAltTime}|\myvec{X}_{\altAltTime-1},\myvec{Y_{\altAltTime}}}
f_{Y_{A^{[\altAltTime]}_{\unit\comma\time}}|\myvec{X}_{\altAltTime-1}}
\right]
\! d\myvec{x}_{0:\time},
\end{equation}
where
$f_{\myvec{X}_{\altAltTime}|\myvec{X}_{\altAltTime-1},\myvec{Y}_{\altAltTime}}$ is called the adapted transition density.
The adapted process (i.e., a stochastic process following the adapted transition density) can be interpreted as a one-step greedy procedure using the data to guide the latent process.
Let 
$g^{}_{\myvec{X}_{0:\Time},\myvec{X}^P_{1:\Time}}(\myvec{x}_{0:\Time},\myvec{x}^P_{1:\Time})$ 
be the joint density of the adapted process and the proposal process, 
\begin{eqnarray}
\nonumber
\hspace{-3mm} g^{}_{\myvec{X}_{0:\Time},\myvec{X}^P_{1:\Time}}(\myvec{x}_{0:\Time},\myvec{x}^P_{1:\Time})
&=& f_{\myvec{X}_0}(\myvec{x}_0) \times 
\\
\label{eq:g}
%\nonumber
&& \hspace{-42mm}
\prod_{\time=1}^{\Time} 
f_{\myvec{X}_{\time}|\myvec{X}_{\time-1},\myvec{Y}_{\time}} 
  \big( 
    \myvec{x}_{\time} \given \myvec{x}_{\time-1},\data{\myvec{y}}_{\time} 
  \big)
\,\,
f_{\myvec{X}_{\time}|\myvec{X}_{\time-1}}
  \big(
    \myvec{x}^P_{\time} \given \myvec{x}_{\time-1}
  \big).
\end{eqnarray}
Using the convention that an empty density $f_{Y_{\emptyset}}$ evaluates to 1, we define
\begin{equation}
%\label{eq:h_S}
\nonumber
\adapted^{}_{\unitTimeSubset}
%(\myvec{x}_{0:\Time})
= 
\prod_{m=1}^{\Time} f_{Y_{\unitTimeSubset^{[m]}}|\myvec{X}^{}_{m-1}} \big( \data{y}_{\unitTimeSubset^{[m]}} \given \myvec{X}^{}_{m-1} \big).
\end{equation}
Denoting $\E_{g}$ for expectation for $(\myvec{X}_{0:\Time},\myvec{X}^P_{1:\Time})$ having density $g_{\myvec{X}_{0:\Time},\myvec{X}^P_{1:\Time}}$, we have $ f_{Y_{A_{u,n}}} = \E_g \big[ \adapted^{}_{A^{}_{\unit\comma\time}}\big]$ and thus
\begin{equation}
%\label{eq:AB:ratio}
\nonumber
f_{Y_{\unit\comma\time}|Y_{A_{\unit\comma\time}}}
= \frac{\E_{g}\big[\adapted^{}_{A^+_{\unit\comma\time}}\big] }{\E_{g}\big[\adapted^{}_{A^{}_{\unit\comma\time}}\big] }.
\end{equation}
Estimating this ratio by Monte Carlo sampling from $g$ is problematic due to the growing size of $A_{\unit\comma\time}$.
Thus, {\ABF} and {\ABFIR} make a localized approximation,
\begin{equation}
\label{eq:AB:ratio2}
 \frac{\E_{g}\big[\adapted^{}_{A^+_{\unit\comma\time}}\big] }{\E_{g}\big[\adapted^{}_{A^{}_{\unit\comma\time}}\big] }
\approx \frac{\E_{g}\big[\adapted^{}_{B^+_{\unit\comma\time}}\big] }{\E_{g}\big[\adapted^{}_{B^{}_{\unit\comma\time}}\big] }.
\end{equation}
The conditional log likelihood estimate $\MC{\loglik}_{\unit\comma\time}$ in {\ABF} and {\ABFIR} come from replacing the expectations on the right hand side of \myeqref{eq:AB:ratio2} with averages over 
Monte Carlo replicates of simulations from the adapted process. 
To see that we expect the approximation in \myeqref{eq:AB:ratio2} to hold when dependence decays across spatiotemporal distance, we can write 
%\begin{equation}
\begin{eqnarray}
\nonumber
\adapted^{}_{A^{}_{\unit\comma\time}} 
&=&
 \adapted^{}_{B^{}_{\unit\comma\time}}  \hspace{1mm} \adapted^{}_{B^{c}_{\unit\comma\time}} 
\\
%\mbox{ and }
\nonumber
\adapted^{}_{A^+_{\unit\comma\time}}
&=&
 \adapted^{}_{B^{+}_{\unit\comma\time}}  \hspace{1mm}  \adapted^{}_{B^{c}_{\unit\comma\time}} ,
%\end{equation}
\end{eqnarray}
where $B^c_{\unit\comma\time}$ is the complement of $B_{\unit\comma\time}$ in $A_{\unit\comma\time}$.
Under our assumptions, the term corresponding to $\adapted_{B^c_{\unit\comma\time}}$ approximately cancels in the numerator and denominator of the right hand side of \myeqref{eq:AB:ratio2}.


The localized likelihood estimate in  {\ABF} and {\ABFIR} has similar structure to {\UBF}.
However,  {\ABF} and {\ABFIR} additionally require the capability to satisfactorily implement adapted simulation.
Adapted simulation is a local calculation, making it an easier task than the global operation of filtering.
Nevertheless, adapted simulation via importance sampling (as carried out by ABF) is vulnerable to COD.
For a continuous time model, the use of intermediate resampling  in ABF-IR is motivated by a result that this can reduce the COD, or avoid it entirely for an ideal guide function \citep{park20}.
Without intermediate resampling even an ideal proposal distribution does not avoid COD for a particle filter \citep{snyder15}.
Assumptions~\ref{B1}--\ref{B:unconditional:mix} below are analogous to ~\ref{A1}--\ref{A:unconditional:mix} and are non-asymptotic assumptions involving $\ethree>0$, $\efourA>0$ and $Q>1$ which are required to hold uniformly over space and [time.
Assumptions~\ref{B:temporal:mix}--\ref{B:XG_XA_ind} control the Monte~Carlo error arising from adapted simulation.
~\ref{B:temporal:mix} is a stability property which asserts that the effect of the latent process on the future of the adapted process decays over time. 
Assumption~\ref{B:girf} is a non-asymptotic bound on Monte~Carlo error for a single step of adapted simulation.
The scaling of the constant $\BvConstant$ with  $\Unit$,  $\Time$ and $\Ninter$ in  Assumption~\ref{B:girf} has been studied by \citet{park20}, where it was established that setting $\Ninter=\Unit$ can lead to $\BvConstant$ being polynomial in $\Unit$ and $\Time$ when using an ideal guide function.
This property is critical to enable ABF-IR to avoid COD. Since ABF has $\Ninter=1$ it suffers from COD, albeit at an empirically slower rate than PF.
The $\efive^{-3}$ error rate in Assumption~\ref{B:girf} follows from balancing the two sources of error defined in the statement of Theorem~2 of \citet{park20} (details are provided in Sec.~{\SuppSecAssumption}).
Assumption~\ref{B:XG_XA_ind} can be guaranteed by the construction of the algorithm, if independently generated Monte~Carlo random variables are used for building the guide function and the one-step prediction particles.
The asymptotic limit in Theorem~\ref{thm:abf} arises as the number of replicates increases.

\Bi %% Assumption~\ref{B1}

\Bii %% Assumption~\ref{B1b}

\Biii %% Assumption~\ref{B2}

\BivA %% Assumption~\ref{B:unconditional:mix}

\BivB %% Assumption~\ref{B:temporal:mix}

\Bv  %% Assumption~\ref{B:girf}

\Bvi    %% Assumption~\ref{B:XG_XA_ind}

\TheoremII %% Theorem~\ref{thm:abf}

\begin{proof}
 A full proof is provided in Sec.~\SuppSecThmII. The extra work to prove Theorem~\ref{thm:abf} beyond the argument for Theorem~\ref{thm:tif} is to bound the error arising from the importance sampling approximation to a draw from the adapted transition density. 
This bound is constructed using Assumptions~\ref{B:temporal:mix}, \ref{B:girf} and~\ref{B:XG_XA_ind}.
The remainder of the proof follows the same approach as Theorem~\ref{thm:tif}, with the adapted process replacing the unconditional latent process.
\end{proof}

The theoretical results foreshadow our empirical observations (Sec.~\ref{sec:examples}) that the relative performance of {\UBF}, {\ABF} and {\ABFIR} is situation-dependent.
Assumption~\ref{A:unconditional:mix} is a mixing assumption for the unconditional latent process, whereas Assumption~\ref{B:unconditional:mix} replaces this with a mixing assumption for the adapted process conditional on the data. 
For a non-stationary process, Assumption~\ref{A:unconditional:mix} may fail to hold uniformly in $\Unit$ whereas the adapted process may provide stable tracking of the latent process (Sec.~\SuppSecAdaptedSimulation).
When Assumption~\ref{A:unconditional:mix} holds, {\UBF} can benefit from not requiring Assumptions~\ref{B:temporal:mix}, \ref{B:girf} and~\ref{B:XG_XA_ind}. 
Adapted simulation is an easier problem than filtering, but nevertheless can become difficult in high dimensions, with the consequence that Assumption~\ref{B:girf} could require large $\BvConstant$.
The tradeoff between {\ABF} and {\ABFIR} depends on the effectiveness of the guide function for the problem at hand.
Intermediate resampling and guide function calculation require additional computational resources, which will necessitate smaller values of $\Rep$ and $\Np$.
In some situations, the improved scaling properties of {\ABFIR} compared to {\ABF}, corresponding to a lower value of $\BvConstant$, will outweigh this cost.

\medskip

\section{Examples}
\label{sec:examples}


We compare the performance of the three bagged filters ({\UBF}, {\ABF} and {\ABFIR}) against each other and against alternative plug-and-play approaches.
The plug-and-play property facilitates numerical implementation for general classes of models, and all the algorithms and models under consideration are implemented in the R packages \code{pomp} \citep{king16} and \code{spatPomp} \citep{asfaw21arxiv}.
Ensemble Kalman filter (EnKF) methods propagate the ensemble members by simulation from the dynamic model and then update the ensemble to assimilate observations using a Gaussian-inspired rule \citep{evensen09book,lei10}.
The block particle filter \citep[BPF,][]{rebeschini15,ng02} partitions the latent space and combines independently drawn components from each partition.
BPF overcomes COD under weak coupling assumptions \citep{rebeschini15}.
Unlike these two methods, our bagged filters modify particles only according to the latent dynamics.
Thus, our methods respect conservation laws and continuity or smoothness conditions obeyed by the dynamic model.
We also compare with a guided intermediate resampling filter \citep[GIRF,][]{park19}, one of many variants of the particle filter designed to scale to larger numbers of units than are possible with a basic particle filter.


First, in Sec.~\ref{sec:bm}, we consider a spatiotemporal Gaussian process for which the exact likelihood is available via a Kalman filter. 
We see in Fig.~\ref{fig:bm_alt_plot} that {\ABFIR} can have a considerable advantages over {\UBF} and {\ABF} for problems with an intermediate level of coupling.
Then, in Sec.~\ref{sec:measles}, we develop a model for measles transmission within and between cities.
The measles model is weakly coupled, leading to successful performance for all three bagged filters.
This class of metapopulation models was the primary motivation for the development of these methodologies.
In Sec.~\ref{sec:profile} we demonstrate an extension from likelihood evaluation to likelihood maximization for the measles model.
Additionally, in  Sec.~\SuppSecLorenz, we compare performance on the Lorenz-96 model, a highly coupled system used to test inference methods for geophysical applications.

\subsection{Correlated Brownian motion}
\label{sec:bm}

Suppose $\myvec{X}(t) = \Omega \myvec{W}(t)$ where $\myvec{W}(t)=W_{1:\Unit}(t)$ comprises $\Unit$ independent standard Brownian motions, and $\Omega_{\unit,\altUnit}=\rho^{\dist(\unit,\altUnit)}$ with $\dist(\unit,\altUnit)$ being the circle distance,
\[
\dist(\unit,\altUnit) 
= \min\big(|\unit-\altUnit|, |\unit-\altUnit+\Unit|, |\unit-\altUnit-\Unit|\big).
\]
Set $t_\time=\time$ for $\time=0,1,\dots,\Time$ with initial value $\myvec{X}(0)=\myvec{0}$ and suppose measurement errors are independent and normally distributed, $Y_{\unit\comma\time}= X_{\unit\comma\time}+ \eta_{\unit\comma\time}$ with
$\eta_{\unit\comma\time}\sim \normal(0,\tau^2)$.
The parameter $\rho$ determines the strength of the spatial coupling. 


%%%%% bbbbbbbbbbbbbbbbbb

<<bm-settings,cache=FALSE,echo=F>>=


bm_files_dir <- paste0("bm_",bm_run_level,"/")
if(!dir.exists(bm_files_dir)) dir.create(bm_files_dir)


stew(file=paste0(bm_files_dir,"bm_settings.rda"),{

  # copy variables that should be included in the stew
  bm_run_level <- bm_run_level 
  bm_cores <- cores

  bm_tol <- 1e-300

  # run_level 1 for debugging; 2 for quick run; 3 for long run; 4 for production(?)

  if(bm_run_level==1){
    bm_U <- c(6, 4)
    bm_N <- 2
    bm_replicates <- 2 # number of Monte Carlo replicates
    bm_girf_Np <- 20
    bm_girf_lookahead <- 2
    bm_girf_nguide <- 10
    bm_pfilter_Np <- 100
    bm_abf_Nrep <- 3
    bm_abf_Np_per_replicate <- 10
    bm_ubf_Nrep <- 3
    bm_abfir_Nrep <- bm_abf_Nrep
    bm_abfir_Np_per_replicate <- bm_abf_Np_per_replicate
    bm_bpf_Np <- 50
    bm_bpf_units_per_block <- 2
    bm_enkf_Np <- 20
  } else if(bm_run_level==2){
    bm_U <- c(32,16,8,4)
    bm_N <- 20
    bm_replicates <- 5 # number of Monte Carlo replicates
    bm_girf_Np <- 1000
    bm_girf_lookahead <- 2
    bm_girf_nguide <- 50
    bm_pfilter_Np <- 10000
    bm_abf_Nrep <- 500
    bm_abf_Np_per_replicate <- 100
    bm_ubf_Nrep <- 5000
    bm_abfir_Nrep <- 200
    bm_abfir_Np_per_replicate <- 50
    bm_bpf_Np <- 5000
    bm_bpf_units_per_block <- 2
    bm_enkf_Np <- 5000
  } else if(bm_run_level==3){
    bm_replicates <- 5 # number of Monte Carlo replicates
    bm_U <- c(40,30,20,10,5)
    bm_N <- 50
    bm_girf_Np <- 1000
    bm_girf_lookahead <- 2
    bm_girf_nguide <- 50
    bm_pfilter_Np <- 50000
    bm_abf_Nrep <- 400
    bm_abf_Np_per_replicate <- 400
    bm_ubf_Nrep <- 10000
    bm_abfir_Nrep <- 200
    bm_abfir_Np_per_replicate <- 200
    bm_bpf_Np <- 10000
    bm_bpf_units_per_block <- 5
    bm_enkf_Np <- 5000    
  } else if(bm_run_level==4){
    bm_replicates <- 5 # number of Monte Carlo replicates
    bm_U <- c(100,80,60,40,30,20,10,5)
    bm_N <- 50
    bm_girf_Np <- 1000
    bm_girf_lookahead <- 2
    bm_girf_nguide <- 50
    bm_pfilter_Np <- 100000
    bm_abf_Nrep <- 400
    bm_abf_Np_per_replicate <- 400
    bm_ubf_Nrep <- 40000
    bm_abfir_Nrep <- 200
    bm_abfir_Np_per_replicate <- 200
    bm_bpf_Np <- 20000
    bm_bpf_units_per_block <- 3
    bm_enkf_Np <- 10000
  }

  set.seed(4512)
  bm_list <- foreach(u=bm_U) %do% bm(U=u, N=bm_N)

})

bm_jobs <- expand.grid(U=bm_U,reps=1:bm_replicates)
bm_jobs$U_id <- rep(seq_along(bm_U),times=bm_replicates)

@



<<bm_nbhd,echo=F>>=
bm_nbhd <- function(object, time, unit) {
  nbhd_list <- list()
  if(time>1) nbhd_list <- c(nbhd_list, list(c(unit, time-1)))
  if(time>2) nbhd_list <- c(nbhd_list, list(c(unit, time-2)))
  if(unit>1) nbhd_list <- c(nbhd_list, list(c(unit-1, time)))
  if(unit>2) nbhd_list <- c(nbhd_list, list(c(unit-2, time)))
  return(nbhd_list)
}
@


<<bm_kf,cache=F,echo=F>>=
bm_kf <- stew(file=paste0(bm_files_dir,"bm_kf.rda"),seed=25487,{
  foreach(i=seq_along(bm_U),.combine=c)%do%{
    bm <- bm_list[[i]]
    bm_units <- bm_U[i]
    bm_dist <- function(u,v,U) min(abs(u-v),abs(u-v+U),abs(u-v-U))
    bm_dmat <- matrix(0,bm_units,bm_units)
    for(u in 1:bm_units) {
      for(v in 1:bm_units) {
        bm_dmat[u,v] <- bm_dist(u,v,bm_units)
      }
    }
    rootQ <- coef(bm)["rho"]^bm_dmat * coef(bm)["sigma"]
    kf <- pomp:::kalmanFilter(bm,
      X0=rinit(bm),
      A= diag(bm_units),
      Q= rootQ %*% rootQ,
      C=diag(bm_units),
      R=diag(coef(bm)["tau"]^2, nrow=bm_units)
    )$logLik
  } -> bm_kf_results
})

bm_jobs$logLik <- rep(bm_kf_results,times=bm_replicates)
@

<<bm_girf,cache=F,echo=F>>=
bm_girf <- stew(file=paste0(bm_files_dir,"bm_girf.rda"),seed=5981724,{
foreach(job=iter(bm_jobs,"row")) %dopar% {
    system.time(
      girf(bm_list[[job$U_id]],
        Np=bm_girf_Np,
        Ninter = job$U,
        kind="moment",
#        kind="bootstrap",
        lookahead = bm_girf_lookahead,
        Nguide = bm_girf_nguide,
        tol = bm_tol) -> bm_girf_out 
    ) -> bm_girf_time
#    uncomment for debugging 
#    list(logLik=logLik(bm_girf_out),time=bm_girf_time["elapsed"],bm_girf_out)
    list(logLik=logLik(bm_girf_out),time=bm_girf_time["elapsed"])
  } -> bm_girf_list
})
bm_jobs$girf_logLik <- vapply(bm_girf_list,function(x)x$logLik,numeric(1))
bm_jobs$girf_time <- vapply(bm_girf_list,function(x) x$time,numeric(1))
@

<<bm_abf,cache=F,echo=F>>=
bm_abf <- stew(file=paste0(bm_files_dir,"bm_abf.rda"),seed=844424,{
  foreach(job=iter(bm_jobs,"row")) %do% {
    system.time(
      abf(bm_list[[job$U_id]], 
        Nrep = bm_abf_Nrep,
        Np=bm_abf_Np_per_replicate,
        nbhd = bm_nbhd, tol=bm_tol) -> bm_abf_out 
    ) -> bm_abf_time
#    uncomment for debugging 
#    list(logLik=logLik(bm_abf_out),time=bm_abf_time["elapsed"],bm_abf_out)
    list(logLik=logLik(bm_abf_out),time=bm_abf_time["elapsed"])
  } -> bm_abf_list
})
bm_jobs$abf_logLik <- vapply(bm_abf_list,function(x)x$logLik,numeric(1))
bm_jobs$abf_time <- vapply(bm_abf_list,function(x) x$time,numeric(1))

@


<<bm_ubf,cache=F,echo=F>>=
bm_ubf <- stew(file=paste0(bm_files_dir,"bm_ubf.rda"),seed=844424,{
  foreach(job=iter(bm_jobs,"row")) %do% {
    system.time(
      abf(bm_list[[job$U_id]], 
        Nrep = bm_ubf_Nrep,
        Np=1,
        nbhd = bm_nbhd, tol=bm_tol) -> bm_ubf_out 
    ) -> bm_ubf_time
#    uncomment for debugging 
#    list(logLik=logLik(bm_ubf_out),time=bm_ubf_time["elapsed"],bm_ubf_out)
    list(logLik=logLik(bm_ubf_out),time=bm_ubf_time["elapsed"])
  } -> bm_ubf_list
})
bm_jobs$ubf_logLik <- vapply(bm_ubf_list,function(x)x$logLik,numeric(1))
bm_jobs$ubf_time <- vapply(bm_ubf_list,function(x) x$time,numeric(1))

@


<<bm_abfir,cache=F,echo=F>>=
bm_abfir <- stew(file=paste0(bm_files_dir,"bm_abfir.rda"),seed=53398,{
  foreach(job=iter(bm_jobs,"row")) %do% {
    system.time(
      abfir(bm_list[[job$U_id]], 
        Nrep = as.integer(bm_abfir_Nrep),
        Np=bm_abfir_Np_per_replicate,
        Ninter = as.integer(job$U/2),
        nbhd = bm_nbhd, tol=bm_tol) -> bm_abfir_out 
    ) -> bm_abfir_time
#    uncomment for debugging 
#    list(logLik=logLik(bm_abfir_out),time=bm_abfir_time["elapsed"],bm_abfir_out)
    list(logLik=logLik(bm_abfir_out),time=bm_abfir_time["elapsed"])
  } -> bm_abfir_list
})
bm_jobs$abfir_logLik <- vapply(bm_abfir_list,function(x)x$logLik,numeric(1))
bm_jobs$abfir_time <- vapply(bm_abfir_list,function(x) x$time,numeric(1))


@


<<bm_pfilter,cache=F,echo=F>>=
bm_pfilter <- stew(file=paste0(bm_files_dir,"bm_pfilter.rda"),seed=53285,{
  foreach(job=iter(bm_jobs,"row")) %dopar% {
    system.time(
      pfilter(bm_list[[job$U_id]],Np=bm_pfilter_Np) -> bm_pfilter_out
    ) -> bm_pfilter_time
#    uncomment for debugging 
#    list(logLik=logLik(bm_pfilter_out),time=bm_pfilter_time["elapsed"],bm_pfilter_out)
    list(logLik=logLik(bm_pfilter_out),time=bm_pfilter_time["elapsed"])
  } -> bm_pfilter_list
})
bm_jobs$pfilter_logLik <- vapply(bm_pfilter_list,function(x)x$logLik,numeric(1))
bm_jobs$pfilter_time <- vapply(bm_pfilter_list,function(x) x$time,numeric(1))
@


	
<<bm_bpf,cache=F,echo=F>>=
#
#  to test:
#  bpfilter(bm_list[[1]],50, block_size=1) -> tmp
#
bm_bpf <- stew(file=paste0(bm_files_dir,"bm_bpf.rda"),seed=53285,{
  foreach(job=iter(bm_jobs,"row")) %dopar% {
    system.time(
      bpfilter(bm_list[[job$U_id]],
        Np=bm_bpf_Np,
	block_size=bm_bpf_units_per_block) -> bm_bpf_out
    ) -> bm_bpf_time
#    uncomment for debugging 
#    list(logLik=logLik(bm_bpf_out),time=bm_bpf_time["elapsed"],bm_bpf_out)
#
# replace using logLik(bm_bpf_out) when that method exists
#
    list(logLik=bm_bpf_out@loglik,time=bm_bpf_time["elapsed"])
  } -> bm_bpf_list
})
bm_jobs$bpf_logLik <- vapply(bm_bpf_list,function(x)x$logLik,numeric(1))
bm_jobs$bpf_time <- vapply(bm_bpf_list,function(x) x$time,numeric(1))
@

<<bm_enkf,cache=F,echo=F>>=
bm_enkf <- stew(file=paste0(bm_files_dir,"bm_enkf.rda"),seed=53285,{
  foreach(job=iter(bm_jobs,"row")) %dopar% {
    system.time(
      enkf(bm_list[[job$U_id]],
        Np=bm_enkf_Np) -> bm_enkf_out
    ) -> bm_enkf_time
    list(logLik=bm_enkf_out@loglik,time=bm_enkf_time["elapsed"])
  } -> bm_enkf_list
})
bm_jobs$enkf_logLik <- vapply(bm_enkf_list,function(x)x$logLik,numeric(1))
bm_jobs$enkf_time <- vapply(bm_enkf_list,function(x) x$time,numeric(1))
@


<<bm_alt_plot, echo=F, fig.height=4, fig.width=5.5, out.width="4.5in", fig.cap = paste('log likelihood estimates for a correlated Brownian motion model of various dimensions. {\\UBF}, {\\ABF} and {\\ABFIR} are compared with a guided intermediate resampling filter (GIRF), standard particle filter (PF), block particle filter (BPF) and ensemble Kalman filter (EnKF). The exact likelihood was computed via a Kalman filter (KF).')>>=


# put output in tall format for plotting
#bm_methods <- c("APF","ABF", "ABFIR","GIRF","KF","PF")
bm_methods <- c("ABF", "ABF-IR","UBF","GIRF","KF","PF","BPF","EnKF")
bm_results <- data.frame(
  Method=rep(bm_methods,each=nrow(bm_jobs)),
  logLik=c(
#    bm_jobs$apf_logLik,
    bm_jobs$abf_logLik,
    bm_jobs$abfir_logLik,
    bm_jobs$ubf_logLik,
    bm_jobs$girf_logLik,
    bm_jobs$logLik,
    bm_jobs$pfilter_logLik,
    bm_jobs$bpf_logLik,
    bm_jobs$enkf_logLik
  ),
  Units=rep(bm_jobs$U,reps=length(bm_methods))
)
bm_point_perturbation <- 1.5
bm_results$U <- bm_results$Units+
  rep( bm_point_perturbation*seq(from=-1,to=1,length=length(bm_methods)),
    each=nrow(bm_jobs))
bm_results$logLik_per_unit <- bm_results$logLik/bm_results$Units
bm_results$logLik_per_obs <- bm_results$logLik_per_unit/bm_N

save(file=paste0(bm_files_dir,"bm_results.rda"),bm_results,bm_jobs)

bm_max <- max(bm_results$logLik_per_obs)

cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(bm_results,mapping = aes(x = U, y = logLik_per_obs, group=Method,color=Method,shape=Method,linetype=Method)
) +
#  geom_point(aes(shape=Method),size=1) +
#  geom_line(aes(linetype=Method), size=1) +
  scale_linetype_manual(values=c(1,1,1,1,2,2,2,2)) +
  scale_shape_manual(values=c(1,2,4,5,1,2,4,5)) +
  scale_color_manual(values=cbPalette[c(2,3,4,5,6,1,7,8)])+
  geom_point() +
  stat_summary(fun=mean, geom="line") +
  coord_cartesian(ylim=c(bm_max-0.35,bm_max))+
  theme(legend.key.width = unit(1,"cm"))+
  theme(axis.text.x = element_text(size = 12),
         axis.text.y = element_text(size = 12),
         axis.title.x = element_text(size = 14),
         axis.title.y = element_text(size = 14),
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank(),
         panel.border = element_rect(colour = "black", fill=NA, size=0.8),
         axis.line = element_line(colour = "black"))+
  ylab("log likelihood per unit per time")

@

Fig.~\ref{fig:bm_alt_plot} shows how the bagged filters scale on this Gaussian model, compared to a standard particle filter (PF), a guided intermediate resampling filter (GIRF), a block particle filter (BPF), and an ensemble Kalman filter.
For our numerical results, we use $\tau=\Sexpr{coef(bm)['tau']}$, $\rho=\Sexpr{coef(bm)['rho']}$ and $\Time=\Sexpr{length(time(bm))}$.
The algorithmic parameters and run times are listed in Sec.~\SuppSecBM, together with a plot of the simulated data and supplementary discussion.
In this case, the exact likelihood is computable via the Kalman filter (KF). 
Since EnKF is based on a Gaussian approximation, it is also exact in this case, up to a small Monte Carlo error. 
The GIRF framework encompasses lookahead particle filter techniques, such as the auxiliary particle filter \citep{pitt99}, and intermediate resampling techniques \citep{delmoral17}. 
GIRF methods combining these techniques were found to perform better than either of these component techniques alone \citep{park20}.
Thus, GIRF here represents a state-of-the-art auxiliary particle filter that targets the complete joint filter density for all units.
We use the general-purpose, plug-and-play implementation of GIRF provided by the \texttt{spatPomp} R package \citep{asfaw21github}; for a Gaussian model, one can calculate an ideal guide function for GIRF but that was not used.
PF works well for small values of $\Unit$ in Fig.~\ref{fig:bm_alt_plot} and rapidly starts struggling as $\Unit$ increases.
GIRF behaves comparably to PF for small $\Unit$ but its performance is maintained for larger $\Unit$.
{\ABF} and {\ABFIR} have some efficiency loss, for small $\Unit$, relative to PF and GIRF due to the localization involved in the filter weighting, but for large $\Unit$ this cost is paid back by the benefit of the reduced Monte Carlo variability.
{\UBF} has a larger efficiency loss for small $\Unit$, but its favorable scaling properties lead it to overtake {\ABF} for larger $\Unit$.
BPF shows stable scaling and modest efficiency loss.
This linear Gaussian SpatPOMP model provides a simple scenario to demonstrate scaling behavior.
For filters that cannot take direct advantage of the Gaussian property of the model, we see that there is a tradeoff between efficiency at low $\Unit$ and scalability.
This is unavoidable, since there is no known algorithm that is simultaneously fully efficient (up to Monte Carlo error), scalable, and applicable to general SpatPOMP models.
We now explore this tradeoff empirically on to a more complex SpatPOMP exemplifying the nonlinear non-Gaussian models motivating our new filtering approach.

%%% mmmmmmmmmmmmmmmmmmmmmm

\subsection{Spatiotemporal measles epidemics}
\label{sec:measles}

Data analysis for spatiotemporal systems featuring nonlinear, nonstationary mechanisms and partial observability has been a longstanding open challenge for ecological and epidemiological analysis \citep{bjornstad01}.
A compartment modeling framework for spatiotemporal population dynamics divides the population at each spatial location into categories, called compartments, which are modeled as homogeneous. 
Spatiotemporal compartment models can be called patch models or metapopulation models in an ecological context.
Ensemble Kalman filter (EnKF) methods provide a state-of-the-art approach to inference for metapopulation models \citep{li20} despite concerns that the approximations inherent in the EnKF can be problematic for models that are highly nonlinear or non-Gaussian \citep{ades15}.
Our bagged filter methodologies have theoretical guarantees for arbitrarily nonlinear and non-Gaussian models, while having improved scaling properties compared to particle filters.

We consider a spatiotemporal model for disease transmission dynamics of measles within and between multiple cities, based on the model of \citet{park20} which adds spatial interaction to the compartment model presented by \cite{he10}.
The model compartmentalizes the population of each city into susceptible ($S$), exposed ($E$), infectious ($I$), and recovered/removed ($R$) categories.
The number of individuals in each compartment city $\unit$ at time $t$ are denoted by integer-valued random variables $S_\unit(t)$, $E_\unit(t)$, $I_\unit(t)$, and $R_\unit(t)$.
The population dynamics are written in terms of counting processes $N_{\bullet\bullet,\unit}(t)$ enumerating cumulative transitions in city $\unit$, up to time $t$, between compartments identified by the subscripts.
We model the $\Unit$ largest cities in the UK, ordered in decreasing size so that $\unit=1$ corresponds to London.
We vary $\Unit$ to test methodologies on a hierarchy of filtering challenges.
Our model is described by the following system of stochastic differential equations, for $\unit=1,\dots, \Unit$,
\begin{equation}
\nonumber
%\label{eq:measles:system}
\begin{array}{lllllll}
\displaystyle dS_\unit(t) &=& dN_{BS,\unit}(t) &-& dN_{SE,\unit}(t) &-& dN_{SD,\unit}(t) \\
\displaystyle dE_\unit(t) &=& dN_{SE,\unit}(t) &-& dN_{EI,\unit}(t) &-& dN_{ED,\unit}(t) \\
\displaystyle dI_\unit(t) &=& dN_{EI,\unit}(t) &-& dN_{IR,\unit}(t) &-& dN_{ID,\unit}(t) 
\end{array}
\end{equation}
Here,  $N_{BS,\unit}(t)$ models recruitment into the susceptible population, and $N_{\bullet D,\unit}(t)$ models emigration and death. 
The total population $P_\unit(t)=S_\unit(t)+E_\unit(t)+I_\unit(t)+R_\unit(t)$ is calculated by smoothing census data and is treated as known.
The number of recovered individuals $R_\unit(t)$ in city $\unit$ is therefore defined implicitly.
$N_{SE,\unit}(t)$ is modeled as negative binomial death processes \citep{breto09,breto11}
with over-dispersion parameter $\sigma_{SE}$, and rate given by
\begin{eqnarray}
\nonumber
\mathbb{E} \big[ N_{SE,\unit}(t+dt) - N_{SE,\unit}(t) \big] 
&=& 
\beta(t) \, S_\unit(t) 
\Big[ 
  \left( \frac{I_\unit+\iota}{P_\unit} \right)^\alpha
\\
\label{eq:dEdt}
&& \hspace{-3.8cm}
 + \sum_{\altUnit \neq \unit} \frac{v_{\unit\altUnit}}{P_\unit} 
  \left\{ 
    \left(
      \frac{ I_{\altUnit} }{ P_{\altUnit} }
    \right)^\alpha - 
    \left(
      \frac{I_\unit}{P_\unit}  
    \right)^\alpha
  \right\}
\Big] dt + o(dt),\label{eqn:transmissionrate}
\end{eqnarray}
where $\beta(t)$ models seasonality driven by high contact rates between children at school, described by
\begin{equation}
\nonumber
  \beta(t)=\begin{cases}
\big(1+\amplitude(1-p)p^{-1} \big)\, \meanBeta & \mbox{ during school term},\\
\big( 1-\amplitude\big) \, \meanBeta& \mbox{ during vacation}
  \end{cases} \label{eq:term}
\end{equation}
with $p = 0.759$ being the proportion of the year taken up by the school terms, $\meanBeta$ is the mean transmission rate, and $\amplitude$ measures the reduction of transmission during school holidays.
In \myeqref{eq:dEdt}, $\alpha$ is a mixing exponent modeling inhomogeneous contact rates within a city, and $\iota$ models immigration of infected individuals which is appropriate when analyzing a subset of cities that cannot be treated as a closed system.
The number of travelers from city $\unit$ to $\altUnit$ is denoted by $v_{\unit\altUnit}$. 
Here, $v_{\unit\altUnit}$ is constructed using the gravity model of \cite{xia04}, 
\[
v_{\unit\altUnit} = \gravity \cdot \frac{\;\overline{\dist}\;}{\bar{P}^2} \cdot \frac{P_\unit \cdot P_{\altUnit}}{\dist(\unit,\altUnit)},
\]
where $\dist(\unit,\altUnit)$ denotes the distance between city $\unit$ and city $\altUnit$, $P_\unit$ is the average population for city $\unit$ across time, $\bar{P}$ is the average population across cities, and $\overline{\dist}$ is the average distance between a randomly chosen pair of cities.
Here, we model $v_{\unit\altUnit}$ as fixed through time and symmetric between any two arbitrary cities, though a natural extension would allow for temporal variation and asymmetric movement between two cities.
The transition processes $N_{EI,\unit}(t)$, $N_{IR,\unit}(t)$ and $N_{\bullet D,\unit}(t)$ are modeled as conditional Poisson processes with per-capita rates $\mu_{EI}$, $\mu_{IR}$ and $\mu_{\bullet D}$ respectively, and we fix $\mu_{\bullet D}=50 \mbox{ year}^{-1}$.
The birth process $N_{BS,\unit}(t)$ is an inhomogeneous Poisson processes with rate $\mu_{BS,\unit}(t)$, given by interpolated census data.

To complete the model specification, we must describe the measurement process.
Let $Z_{\unit\comma\time}=N_{IR\comma\unit}(t_\time)-N_{IR\comma\unit}(t_{\time-1})$ be the number of removed infected individuals in the $n$th reporting interval.
Suppose that cases are quarantined once they are identified, so that reported cases comprise a fraction $\rho$ of these removal events.
The case report $\data{y}_{\unit\comma\time}$ is modeled as a realization of a discretized conditionally Gaussian random variable $Y_{\unit\comma\time}$, defined for $y>0$ via
\begin{eqnarray}
\nonumber
\prob\big[Y_{\unit\comma\time}{=}y\mid Z_{\unit\comma\time}{=}z\big] &=& \Phi\big(y+0.5; \rho z,\rho(1-\rho)z+\psi^2\rho^2z^2\big)
\\
&&\hspace{-20mm}
- \Phi\big(y-0.5; \rho z,\rho(1-\rho)z+\psi^2\rho^2z^2\big)
\label{eq:obs}
\end{eqnarray}
where $\Phi(\cdot;\mu,\sigma^2)$ is the $\normal(\mu,\sigma^2)$ cumulative distribution function, and $\psi$ models overdispersion relative to the binomial distribution.
For $y=0$, we replace $y-0.5$ by $-\infty$ in \myeqref{eq:obs}.


%%%%%%  msmsmsmsmsmsmsmsmsmsms %%%%%%%%%%%%

<<measles_spatPomp,eval=T,echo=F>>=
library(spatPomp)
measles_model_dir <- "measlesModel/"
if(!dir.exists(measles_model_dir)) dir.create(measles_model_dir)

measles_spatPomp <- stew(file=paste0(measles_model_dir,"measles_spatPomp.rda"),{
  measles_U <- 40    # number of cities (units) for simulation that can subsequently be subsetted
  measles_N <- 15*26 # number of 2-week observation intervals for simulation that can subsequently be subsetted
  measles_unit_statenames <- c('S','E','I','R', 'C','W')
  measles_S_0 <- 0.032; measles_E_0 <- 0.00005; measles_I_0 <- 0.00004
  measles_unit_params <- c(
    alpha=1,
    iota=0,  # set to zero for a closed population
    R0=30,
    cohort=0,
    amplitude=0.5, gamma=52, sigma=52,mu=0.02,
    sigmaSE=0.15, rho=0.5,
    psi=0.15,
    g=400,
    S_0=measles_S_0, E_0=measles_E_0, I_0=measles_I_0,
    R_0=1-measles_S_0-measles_E_0-measles_I_0
  )
  measles_uk <- measles(measles_U)
  measles_statenames <- paste0(rep(measles_unit_statenames,
    each=measles_U),1:measles_U)
  measles_IVPnames <- paste0(measles_statenames[1:(4*measles_U)],"_0")
  measles_RPnames <- c("alpha","iota","R0","cohort","amplitude",
    "gamma","sigma","mu","sigmaSE","rho","psi","g")
  measles_paramnames <- c(measles_RPnames,measles_IVPnames)

  measles_params <- rep(NA,length(measles_paramnames))
  names(measles_params) <- measles_paramnames

  measles_params[measles_RPnames] <- measles_unit_params[measles_RPnames]
  measles_params[paste0("S",1:measles_U,"_0")] <-measles_unit_params["S_0"]
  measles_params[paste0("E",1:measles_U,"_0")] <-measles_unit_params["E_0"]
  measles_params[paste0("I",1:measles_U,"_0")] <-measles_unit_params["I_0"]
  measles_params[paste0("R",1:measles_U,"_0")] <-measles_unit_params["R_0"]

  set.seed(34)
  measles_sim <- simulate(measles_uk,params=measles_params,
    times=time(measles_uk)[1:measles_N])

  measles_subset <- function(m_U,m_N){
    m <- measles(U=m_U)
    m@data <- measles_sim@data[1:m_U,1:m_N]
    time(m) <- measles_sim@times[1:m_N]
    m_statenames <- paste0(rep(measles_unit_statenames,each=m_U),1:m_U)
    m_IVPnames <- paste0(m_statenames[1:(4*m_U)],"_0")
    m_paramnames <- c(measles_RPnames,m_IVPnames)
    m_params <- measles_params[names(measles_params)%in%m_paramnames]
    coef(m) <- m_params
    return(m)
  }
})
@



<<measles_image_plot, echo=F, fig.height=6, fig.width=7, out.width="6.5in", fig.cap = paste('Log(reported cases $+$ 1) for (A) the measles simulation used for the likelihood slice; (B) the corresponding UK measles data. The simulation shares the biennial pattern, with most but not all cities locked in phase most of the time.')>>=
library(spatPomp)
library(fields)
par(mai=c(0.5,0.7,0.1,0.1))
par(mfrow=c(2,1))
image.plot(y=1:dim(obs(measles_sim))[1],x=time(measles_sim),z=log(t(obs(measles_sim)+1)),ylab="unit",xlab="",col=gray.colors(33))
mtext("A",side=2,line=2.5,las=1,padj=-5.5,cex=1.8)
image.plot(y=1:dim(obs(measles_uk))[1],x=time(measles_uk),z=log(t(obs(measles_uk)+1)),ylab="unit",xlab="",col=gray.colors(33))
mtext("B",side=2,line=2.5,las=1,padj=-5.5,cex=1.8)
mtext("time",side=1,line=2.5)
@

<<mscale_settings,cache=FALSE,echo=F>>=

  # run_level 1 for debugging; 2 for quick run; 3 for long run;
  # 4 even longer; 5 for production(?)
  # 6 for a small-U test figure

mscale_files_dir <- paste0("mscale_",mscale_run_level,"/")
if(!dir.exists(mscale_files_dir)) dir.create(mscale_files_dir)

stew(file=paste0(mscale_files_dir,"mscale_settings.rda"),{

  # copy variables that should be included in the stew
  mscale_run_level <- mscale_run_level 
  mscale_cores <- cores

  mscale_tol <- 1e-300

  if(mscale_run_level==1){
    mscale_U <- c(4, 2)
    mscale_N <- 5
    mscale_replicates <- 2 # number of Monte Carlo replicates
    mscale_girf_Np <- 50
    mscale_girf_lookahead <- 2
    mscale_girf_nguide <- 10
    mscale_pfilter_Np <- 100
    mscale_abf_Nrep <- 3
    mscale_abf_Np_per_replicate <- 10
    mscale_ubf_Nrep <- 20
    mscale_abfir_Nrep <- mscale_abf_Nrep
    mscale_abfir_Np_per_replicate <- mscale_abf_Np_per_replicate
    mscale_enkf_Np <- 100
    mscale_bpf_units_per_block <- 1
    mscale_bpf_Np <- 50
    mscale_bootgirf_Np <- 50
    mscale_bootgirf_nguide <- 10
    mscale_bootgirf_lookahead <- 2
  } else if(mscale_run_level==2){
    mscale_U <- c(16,8,4,2)
    mscale_N <- 4*26
    mscale_replicates <- 4 # number of Monte Carlo replicates
    mscale_girf_Np <- 1000
    mscale_girf_lookahead <- 1
    mscale_girf_nguide <- 50
    mscale_pfilter_Np <- 10000
    mscale_abf_Nrep <- 500
    mscale_abf_Np_per_replicate <- 100
    mscale_ubf_Nrep <- 5000
    mscale_abfir_Nrep <- 200
    mscale_abfir_Np_per_replicate <- 50
    mscale_enkf_Np <- 5000
    mscale_bpf_units_per_block <- 1
    mscale_bpf_Np <- 5000
    mscale_bootgirf_Np <- 1000
    mscale_bootgirf_nguide <- 20
    mscale_bootgirf_lookahead <- 2
  } else if(mscale_run_level==3){
    mscale_replicates <- 5 # number of Monte Carlo replicates
    mscale_U <- c(32, 16, 8, 4, 2)
    mscale_N <- 5*26
    mscale_girf_Np <- 2000
    mscale_girf_lookahead <- 2
    mscale_girf_nguide <- 20
    mscale_pfilter_Np <- 50000
    mscale_abf_Nrep <- 500
    mscale_abf_Np_per_replicate <- 100
    mscale_ubf_Nrep <- 5000
    mscale_abfir_Nrep <- 200
    mscale_abfir_Np_per_replicate <- 100
    mscale_enkf_Np <- 10000
    mscale_bpf_units_per_block <- 1
    mscale_bpf_Np <- 10000
    mscale_bootgirf_Np <- 2000
    mscale_bootgirf_nguide <- 20
    mscale_bootgirf_lookahead <- 2
  } else if(mscale_run_level==4){
    mscale_replicates <- 5 # number of Monte Carlo replicates
    mscale_U <- c(20,16,12,10,8,6,4,2)
    mscale_N <- 52
    mscale_girf_Np <- 2000
    mscale_girf_lookahead <- 1
    mscale_girf_nguide <- 50
    mscale_pfilter_Np <- 100000
    mscale_abf_Nrep <- 500
    mscale_abf_Np_per_replicate <- 500
    mscale_ubf_Nrep <- 10000
    mscale_abfir_Nrep <- 200
    mscale_abfir_Np_per_replicate <- 200
    mscale_enkf_Np <- 10000
    mscale_bpf_units_per_block <- 1
    mscale_bpf_Np <- 50
  } else if(mscale_run_level==5){
    mscale_replicates <- 5 # number of Monte Carlo replicates
    mscale_U <- c(40,36,32,28,24,20,16,14,12,10,8,6,4,2)
    mscale_N <- 5*26
    mscale_girf_Np <- 2000
    mscale_girf_lookahead <- 1
    mscale_girf_nguide <- 40
    mscale_pfilter_Np <- 100000
    mscale_abf_Nrep <- 500
    mscale_abf_Np_per_replicate <- 500
    mscale_ubf_Nrep <- 20000
    mscale_abfir_Nrep <- 200
    mscale_abfir_Np_per_replicate <- 200
    mscale_enkf_Np <- 10000
    mscale_bpf_units_per_block <- 4
    mscale_bpf_Np <- 20000
  }  else if(mscale_run_level==6){
    mscale_replicates <- 5 # number of Monte Carlo replicates
    mscale_U <- c(8,6,4,3,2)
    mscale_N <- 52
    mscale_girf_Np <- 2000
    mscale_girf_lookahead <- 1
    mscale_girf_nguide <- 50
    mscale_pfilter_Np <- 100000
    mscale_abf_Nrep <- 500
    mscale_abf_Np_per_replicate <- 200
    mscale_ubf_Nrep <- 40000
#    mscale_abfir_Nrep <- 200
#    mscale_abfir_Np_per_replicate <- 200
    mscale_abfir_Nrep <- 10
    mscale_abfir_Np_per_replicate <- 10
    mscale_enkf_Np <- 1000
    mscale_bpf_units_per_block <- 1
    mscale_bpf_Np <- 2000
  } 

})

mscale_jobs <- expand.grid(U=mscale_U,reps=1:mscale_replicates)
mscale_jobs$U_id <- rep(seq_along(mscale_U),times=mscale_replicates)

@

<<mscale_spatPomp,eval=T,echo=F>>=
mscale_spatPomp <- stew(file=paste0(mscale_files_dir,"mscale_spatPomp.rda"),{

  mscale_model_dir <-  measles_model_dir
  # note: care is required if mscale_model_dir is set to something other than measles_model_dir
  # however, it may be useful to do that while testing model variations.
  
  load(file=paste0(mscale_model_dir,"measles_spatPomp.rda"))  
  mscale_list <- vector("list", length = length(mscale_U))  
  for(i in 1:length(mscale_U)) mscale_list[[i]] <- measles_subset(m_U=mscale_U[i], m_N=mscale_N)
})
@

<<mscale_nbhd,echo=F>>=
mscale_nbhd <- function(object, time, unit) {
  nbhd_list <- list()
  if(time>1) nbhd_list <- c(nbhd_list, list(c(unit, time-1)))
  if(time>2) nbhd_list <- c(nbhd_list, list(c(unit, time-2)))
  return(nbhd_list)
}
@

<<mscale_girf,cache=F,echo=F>>=
mscale_girf <- stew(file=paste0(mscale_files_dir,"mscale_girf.rda"),seed=5981724,{
  foreach(job=iter(mscale_jobs,"row")) %dopar% {
    system.time(
      girf(mscale_list[[job$U_id]],
        kind="moment",
        Np=mscale_girf_Np,
        Ninter = job$U,
        lookahead = mscale_girf_lookahead,
        Nguide = mscale_girf_nguide,
        tol = mscale_tol) -> mscale_girf_out 
    ) -> mscale_girf_time
#    uncomment for debugging 
#    list(logLik=logLik(mscale_girf_out),time=mscale_girf_time["elapsed"],mscale_girf_out)
    list(logLik=logLik(mscale_girf_out),time=mscale_girf_time["elapsed"])
  } -> mscale_girf_list
})
mscale_jobs$girf_logLik <- vapply(mscale_girf_list,function(x)x$logLik,numeric(1))
mscale_jobs$girf_time <- vapply(mscale_girf_list,function(x) x$time,numeric(1))
@

<<mscale_abf,cache=F,echo=F>>=
mscale_abf <- stew(file=paste0(mscale_files_dir,"mscale_abf.rda"),seed=844424,{
  foreach(job=iter(mscale_jobs,"row")) %do% {
    system.time(
      abf(mscale_list[[job$U_id]], 
        Nrep = mscale_abf_Nrep,
        Np=mscale_abf_Np_per_replicate,
        nbhd = mscale_nbhd, tol=mscale_tol) -> mscale_abf_out 
    ) -> mscale_abf_time
#    uncomment for debugging 
#    list(logLik=logLik(mscale_abf_out),time=mscale_abf_time["elapsed"],mscale_abf_out)
    list(logLik=logLik(mscale_abf_out),time=mscale_abf_time["elapsed"])
  } -> mscale_abf_list
})
mscale_jobs$abf_logLik <- vapply(mscale_abf_list,function(x)x$logLik,numeric(1))
mscale_jobs$abf_time <- vapply(mscale_abf_list,function(x) x$time,numeric(1))

@

<<mscale_ubf,cache=F,echo=F>>=
mscale_ubf <- stew(file=paste0(mscale_files_dir,"mscale_ubf.rda"),seed=844424,{
  foreach(job=iter(mscale_jobs,"row")) %do% {
    system.time(
      abf(mscale_list[[job$U_id]], 
        Nrep = mscale_ubf_Nrep,
        Np=1,
        nbhd = mscale_nbhd, tol=mscale_tol) -> mscale_ubf_out 
    ) -> mscale_ubf_time
#    uncomment for debugging 
#    list(logLik=logLik(mscale_ubf_out),time=mscale_ubf_time["elapsed"],mscale_ubf_out)
    list(logLik=logLik(mscale_ubf_out),time=mscale_ubf_time["elapsed"])
  } -> mscale_ubf_list
})
mscale_jobs$ubf_logLik <- vapply(mscale_ubf_list,function(x)x$logLik,numeric(1))
mscale_jobs$ubf_time <- vapply(mscale_ubf_list,function(x) x$time,numeric(1))

@

<<mscale_abfir,cache=F,echo=F>>=
mscale_abfir <- stew(file=paste0(mscale_files_dir,"mscale_abfir.rda"),seed=53398,{
  foreach(job=iter(mscale_jobs,"row")) %do% {
    system.time(
      abfir(mscale_list[[job$U_id]], 
        Nrep = as.integer(mscale_abfir_Nrep),
        Np=mscale_abfir_Np_per_replicate,
        Ninter = as.integer(job$U/2),
        nbhd = mscale_nbhd, tol=mscale_tol) -> mscale_abfir_out 
    ) -> mscale_abfir_time
#    uncomment for debugging 
#    list(logLik=logLik(mscale_abfir_out),time=mscale_abfir_time["elapsed"],mscale_abfir_out)
    list(logLik=logLik(mscale_abfir_out),time=mscale_abfir_time["elapsed"])
  } -> mscale_abfir_list
})
mscale_jobs$abfir_logLik <- vapply(mscale_abfir_list,function(x)x$logLik,numeric(1))
mscale_jobs$abfir_time <- vapply(mscale_abfir_list,function(x) x$time,numeric(1))

@


<<mscale_pfilter,cache=F,echo=F>>=
mscale_pfilter <- stew(file=paste0(mscale_files_dir,"mscale_pfilter.rda"),seed=53285,{
  foreach(job=iter(mscale_jobs,"row")) %dopar% {
    system.time(
      pfilter(mscale_list[[job$U_id]],Np=mscale_pfilter_Np) -> mscale_pfilter_out
    ) -> mscale_pfilter_time
#    uncomment for debugging 
#    list(logLik=logLik(mscale_pfilter_out),time=mscale_pfilter_time["elapsed"],mscale_pfilter_out)
    list(logLik=logLik(mscale_pfilter_out),time=mscale_pfilter_time["elapsed"])
  } -> mscale_pfilter_list
})
mscale_jobs$pfilter_logLik <- vapply(mscale_pfilter_list,function(x)x$logLik,numeric(1))
mscale_jobs$pfilter_time <- vapply(mscale_pfilter_list,function(x) x$time,numeric(1))
@


<<mscale_bpf,cache=F,echo=F>>=
mscale_bpf <- stew(file=paste0(mscale_files_dir,"mscale_bpf.rda"),seed=53285,{
  foreach(job=iter(mscale_jobs,"row")) %dopar% {
    system.time(
      bpfilter(mscale_list[[job$U_id]],
        Np=mscale_bpf_Np,
	block_size=mscale_bpf_units_per_block) -> mscale_bpf_out
    ) -> mscale_bpf_time
#    uncomment for debugging 
#    list(logLik=logLik(mscale_bpf_out),time=mscale_bpf_time["elapsed"],mscale_bpf_out)
    list(logLik=mscale_bpf_out@loglik,time=mscale_bpf_time["elapsed"])
  } -> mscale_bpf_list
})
mscale_jobs$bpf_logLik <- vapply(mscale_bpf_list,function(x)x$logLik,numeric(1))
mscale_jobs$bpf_time <- vapply(mscale_bpf_list,function(x) x$time,numeric(1))
@


<<mscale_enkf,cache=F,echo=F>>=
###  genkf(mscale_list[[1]],Np=mscale_enkf_Np) -> mscale_enkf_out
mscale_enkf <- stew(file=paste0(mscale_files_dir,"mscale_enkf.rda"),seed=53285,{
  foreach(job=iter(mscale_jobs,"row")) %dopar% {
    system.time(
      enkf(mscale_list[[job$U_id]],Np=mscale_enkf_Np) -> mscale_enkf_out
    ) -> mscale_enkf_time
#    uncomment for debugging 
#    list(logLik=logLik(mscale_enkf_out),time=mscale_enkf_time["elapsed"],mscale_enkf_out)
    list(logLik=logLik(mscale_enkf_out),time=mscale_enkf_time["elapsed"])
  } -> mscale_enkf_list
})
mscale_jobs$enkf_logLik <- vapply(mscale_enkf_list,function(x)x$logLik,numeric(1))
mscale_jobs$enkf_time <- vapply(mscale_enkf_list,function(x) x$time,numeric(1))
@



<<mscale_loglik_plot, echo=F,  fig.height=3, fig.width=4.5, out.width="4in", fig.cap = paste('log likelihood estimates for simulated data from the measles model of various dimensions. {\\UBF}, {\\ABF} and {\\ABFIR} are compared with a guided intermediate resampling filter (GIRF), a standard particle filter (PF), a block particle filter (BPF) and an ensemble Kalman filter (EnKF).')>>=


# put output in tall format for plotting

mscale_methods <- c("ABF", "ABF-IR","UBF","GIRF","PF","BPF","EnKF")
# mscale_methods <- c("ABF", "ABF-IR","UBF","GIRF","PF","BPF","EnKF","bootgirf")
mscale_results <- data.frame(
  Method=rep(mscale_methods,each=nrow(mscale_jobs)),
  logLik=c(
    mscale_jobs$abf_logLik,
    mscale_jobs$abfir_logLik,
    mscale_jobs$ubf_logLik,
    mscale_jobs$girf_logLik,
    mscale_jobs$pfilter_logLik,
    mscale_jobs$bpf_logLik,
    mscale_jobs$enkf_logLik
#    mscale_jobs$bootgirf_logLik
  ),
  Units=rep(mscale_jobs$U,reps=length(mscale_methods))
)
mscale_point_perturbation <- 0.25
mscale_results$U <- mscale_results$Units+
  rep( mscale_point_perturbation*seq(from=-1,to=1,length=length(mscale_methods)),
    each=nrow(mscale_jobs))
mscale_results$logLik_per_unit <- mscale_results$logLik/mscale_results$Units
mscale_results$logLik_per_obs <- mscale_results$logLik_per_unit/mscale_N

save(file=paste0(mscale_files_dir,"mscale_results.rda"),mscale_results,mscale_jobs)

mscale_max <- max(mscale_results$logLik_per_obs)

mscale_linetype <- c(1,1,1,1,2,2,2,3)
mscale_shape <- c(1,2,4,5,1,2,4,3)
#mscale_color <- cbPalette[c(2:8,1)]
mscale_color <- cbPalette[2:8]
# mscale_color <- c(1,2,3,4,5,6,8)

ggplot(mscale_results,mapping = aes(x = U, y = logLik_per_obs, group=Method,color=Method,linetype=Method,shape=Method)) +
  scale_linetype_manual(values=mscale_linetype) +
  scale_shape_manual(values=mscale_shape) +
  scale_color_manual(values=mscale_color)+
  geom_point() +
  stat_summary(fun=mean, geom="line") +
  coord_cartesian(ylim=c(mscale_max-2.5,mscale_max))+
  theme(legend.key.width = unit(1,"cm"))+
  theme(axis.text.x = element_text(size = 12),
         axis.text.y = element_text(size = 12),
         axis.title.x = element_text(size = 14),
         axis.title.y = element_text(size = 14),
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank(),
         panel.border = element_rect(colour = "black", fill=NA, size=0.8),
         axis.line = element_line(colour = "black"))+
  ylab("log likelihood per unit per time")

@



This model includes many features that have been proposed to be relevant for understanding measles transmission dynamics \citep{he10}.
Our plug-and-play methodology permits consideration of all these features, and readily extends to the investigation of further variations. 
Likelihood-based inference via plug-and-play methodology therefore provides a framework for evaluating which features of a dynamical model are critical for explaining the data \citep{king08}.
By contrast, \cite{xia04} developed a linearization for a specific spatiotemporal measles model which is numerically convenient but not readily adaptable to assess alternative model choices.
Fig.~\ref{fig:measles_image_plot} shows a simulation from our model, showing that trajectories from this model can capture some features of the system that have been hard to understand: how can it be that disease transmission dynamics between locations have important levels of interaction yet are not locked in synchrony \citep{becker20}?
Here, we are testing statistical tools rather than engaging directly in the scientific debate so we test methods on the simulated data.

We first assess the scaling properties of the filters on the measles model by evaluating the likelihood over varying numbers of units, $\Unit$, for fixed parameters.
The results are given in Fig.~\ref{fig:mscale_loglik_plot}, with additional information about timing, algorithmic choices, parameter values and a plot of the data provided in Sec.~\SuppSecMeasles.
In Fig.~\ref{fig:mscale_loglik_plot}, the log likelihood per unit per time increases with $\Unit$ because city size decreases with $\Unit$. 
Smaller cities have fewer measles cases, resulting in a narrower and taller probability density function.
Fig.~\ref{fig:mscale_loglik_plot} shows a rapid decline in the performance of the particle filter (PF) beyond $\Unit=4$.
This is a challenging filtering problem, with dynamics including local fadeouts and high stochasticity in each city stabilized at the metapopulation level by the coupling.
In this example, GIRF performs poorly suggesting that the simulated moment guide function is less than successful.
We used the general-purpose implementation of GIRF in the \code{spatPomp} package, and there might be room for improvement by developing a model-specific guide function.
{\ABFIR} uses the same guide function, and this may explain why {\ABFIR} performs worse than {\ABF} here, though {\ABFIR} is much less sensitive than GIRF to the quality of the guide.
{\ABF} and {\UBF} are competing with BPF as winners on this challenge.
The bagged filters and BPF have substantial advantages compared to EnKF, amounting to more than 0.2 log likelihood units per observation.
We suspect that the limitations of EnKF on this problem are due to the nonlinearity, non-Gaussianity, and discreteness of fadeout and reintroduction dynamics.
Thus, EnKF is relatively effective with small ensemble size but soon reaches the limit of its capabilities (Sec.~{\SuppSecFixedU}).
By contrast, the bagged filters and block particle filter perform substantially better than EnKF for larger ensemble size (Sec.~{\SuppSecFixedU}).
All the algorithms have various other tuning parameters that could influence the results.
Some investigations of alternatives are presented in Secs.~{\SuppSecMeasles},~{\SuppSecMeaslesNbhd} and~{\SuppSecIvsJ}.
Generalizable conclusions are hard to infer from numerical comparisons of complex algorithms on complex models.
Experimentation with different methods, and their tuning parameters, is recommended when investigating a new model.


<<slice_settings,echo=F,eval=T>>=

slice_files_dir <- paste0("slice_",slice_run_level,"/")
if(!dir.exists(slice_files_dir)) dir.create(slice_files_dir)

stew(file=paste0(slice_files_dir,"slice_settings.rda"),{

  # copy variables that should be included in the stew
  slice_run_level <- slice_run_level 
  slice_cores <- cores

  g_lo <- 200
  g_hi <- 600

  g_bpf_lo <- g_lo
  g_bpf_hi <- g_hi

#  g_enkf_lo <- 50
#  g_enkf_hi <- 750  

   g_enkf_lo <- g_lo
   g_enkf_hi <- g_hi

  if(slice_run_level==1){
    slice_Ntheta <- 5
    slice_U <- 4
    slice_N <- 3
    slice_replicates <- 2
    slice_abf_Nrep <- 3
    slice_abf_Np_per_replicate <- 10
    slice_bpf_units_per_block <- 1
    slice_bpf_Np <- 100
    slice_ubf_Nrep <- 5
    slice_enkf_Np <- 20
}


  if(slice_run_level==2){
    slice_Ntheta <- 10
    slice_U <- 20
    slice_N <- 10*26
    slice_replicates <- 2 # number of Monte Carlo replicates
    slice_abf_Nrep <- 200
    slice_abf_Np_per_replicate <- 50
    slice_bpf_units_per_block <- 1
    slice_bpf_Np <- 5000
    slice_ubf_Nrep <- 5000
    slice_enkf_Np <- 5000
  }

  if(slice_run_level==3){
    slice_Ntheta <- 10
    slice_replicates <- 4 # number of Monte Carlo replicates
    slice_U <- 40
    slice_N <- 15*26
    slice_abf_Nrep <- 1000
    slice_abf_Np_per_replicate <- 100
    slice_bpf_units_per_block <- 1
    slice_bpf_Np <- 40000
    slice_ubf_Nrep <- 40000
    slice_enkf_Np <- 25000
  }

  if(slice_run_level==4){
    slice_Ntheta <- 12
    slice_replicates <- 5 # number of Monte Carlo replicates
    slice_U <- 40
    slice_N <- 15*26
    slice_abf_Nrep <- 5000
    slice_abf_Np_per_replicate <- 100
    slice_bpf_units_per_block <- 2
    slice_bpf_Np <- 40000
    slice_ubf_Nrep <- 40000
    slice_enkf_Np <- 5000
  }


})
@

<<slice_spatPomp,eval=T,echo=F>>=
slice_spatPomp <- stew(file=paste0(slice_files_dir,"slice_spatPomp.rda"),{

  slice_model_dir <-  measles_model_dir
  # note: care is required if slice_model_dir is set to something other than measles_model_dir
  # however, it may be useful to do that while testing model variations.
  
  load(file=paste0(slice_model_dir,"measles_spatPomp.rda"))
  slice_sim <- measles_subset(m_U=slice_U, m_N=slice_N)

  slice_sliceJobs <- slice_design(
    center=coef(slice_sim),
    g=rep(seq(from=g_lo,to=g_hi,length=slice_Ntheta),each=slice_replicates)
  )

  slice_bpf_sliceJobs <- slice_design(
    center=coef(slice_sim),
    g=rep(seq(from=g_bpf_lo,to=g_bpf_hi,length=slice_Ntheta),each=slice_replicates)
  )

  slice_enkf_sliceJobs <- slice_design(
    center=coef(slice_sim),
    g=rep(seq(from=g_enkf_lo,to=g_enkf_hi,length=slice_Ntheta),each=slice_replicates)
  )

})
@


<<slice_nbhd,echo=F>>=
slice_nbhd <- function(object, time, unit) {
  nbhd_list <- list()
  if(time>1) nbhd_list <- c(nbhd_list, list(c(unit, time-1)))
  if(time>2) nbhd_list <- c(nbhd_list, list(c(unit, time-2)))
  return(nbhd_list)
}
@



<<slice_abf, eval=T, cache=F,echo=F>>=
slice_abf_files <- paste0(slice_files_dir,"slice_abf",1:nrow(slice_sliceJobs),".rda")
# parallelization is carried out within abf so we use a serial foreach
foreach (slice_row=1:nrow(slice_sliceJobs), .combine=rbind) %do% {
  slice_abf <- stew(file=slice_abf_files[slice_row],seed=4000817+slice_row,{
    theta <- unlist(slice_sliceJobs[slice_row,names(coef(slice_sim))])
    system.time(
      abf(slice_sim,params=theta, 
        Nrep = slice_abf_Nrep,
        Np=slice_abf_Np_per_replicate,
        nbhd = slice_nbhd,tol=1e-300
      ) -> abf1
    ) -> slice_abf_time
    abf_logLik <- logLik(abf1)
    rm(abf1)
  })
  c(logLik=abf_logLik,theta, Np=slice_abf_Np_per_replicate,
    Nrep = slice_abf_Nrep, U=length(unit_names(slice_sim)), N=length(time(slice_sim)),
    time=slice_abf_time["elapsed"]
  )
} -> slice_abf_results

@

<<slice_ubf, eval=T, cache=F,echo=F>>=
slice_ubf_files <- paste0(slice_files_dir,"slice_ubf",1:nrow(slice_sliceJobs),".rda")
# parallelization is carried out within ubf so we use a serial foreach
foreach (slice_row=1:nrow(slice_sliceJobs), .combine=rbind) %do% {
  slice_ubf <- stew(file=slice_ubf_files[slice_row],seed=4000817+slice_row,{
    theta <- unlist(slice_sliceJobs[slice_row,names(coef(slice_sim))])
    system.time(
      abf(slice_sim,params=theta, 
        Nrep = slice_ubf_Nrep,
        Np=1,
        nbhd = slice_nbhd,tol=1e-300
      ) -> ubf1
    ) -> slice_ubf_time
    ubf_logLik <- logLik(ubf1)
    rm(ubf1)
  })
  c(logLik=ubf_logLik,theta,
    Nrep = slice_ubf_Nrep, U=length(unit_names(slice_sim)), N=length(time(slice_sim)),
    time=slice_ubf_time["elapsed"]
  )
} -> slice_ubf_results    

@



<<slice_abfir,eval=F,cache=F,echo=F>>=
slice_abfir_files <- paste0(slice_files_dir,"slice_abfir",1:nrow(slice_sliceJobs),".rda")
# parallelization is carried out within abfir so we use a serial foreach
foreach (slice_row=1:nrow(slice_sliceJobs), .combine=rbind) %do% {  
  slice_abfir <- stew(file=slice_abfir_files[slice_row],seed=4000817+slice_row,{
    theta <- unlist(slice_sliceJobs[slice_row,names(coef(slice_sim))])
    system.time(
      abfir(slice_sim,params=theta,
        Nrep = slice_abfir_Nrep,
        Np=slice_abfir_Np_per_replicate,
        nbhd = slice_nbhd,
        Ninter = as.integer(slice_U/2),
        tol = 1e-300
      ) -> abfir1
    ) -> slice_abfir_time
    abfir_logLik <- logLik(abfir1)
    rm(abfir1)
  })    
  c(logLik=abfir_logLik,theta, Np=slice_abfir_Np_per_replicate,
      Nrep = slice_abfir_Nrep, U=length(unit_names(slice_sim)), N=length(time(slice_sim)),
      time=slice_abfir_time["elapsed"]
  )
} -> slice_abfir_results 
@

<<slice_bpf,cache=F,echo=F,eval=T>>=
slice_bpf_files <- paste0(slice_files_dir,"slice_bpf",1:nrow(slice_bpf_sliceJobs),".rda")
foreach (slice_row=1:nrow(slice_bpf_sliceJobs), .combine=rbind) %dopar% {
  theta_eval <<- unlist(slice_bpf_sliceJobs[slice_row,names(coef(slice_sim))])
  stew(file=slice_bpf_files[slice_row],seed=4000817+slice_row,{
    theta <- theta_eval
    system.time(
      bpfilter(slice_sim,params=theta,
        Np=slice_bpf_Np,
	block_size=slice_bpf_units_per_block
      ) -> slice_bpf_out
    ) -> slice_bpf_time
    bpf_logLik <- slice_bpf_out@loglik
    rm(slice_bpf_out)
  })
  c(logLik=bpf_logLik,theta, Np=slice_bpf_Np,
    units_per_block = slice_bpf_units_per_block,
    U=length(unit_names(slice_sim)), N=length(time(slice_sim)),
    time=slice_bpf_time["elapsed"]
  )
} -> slice_bpf_results    
@


<<slice_enkf,cache=F,echo=F,eval=T>>=

slice_enkf_files <- paste0(slice_files_dir,"slice_enkf",1:nrow(slice_enkf_sliceJobs),".rda")
foreach (slice_row=1:nrow(slice_enkf_sliceJobs), .combine=rbind) %dopar% {
  theta_eval <<- unlist(slice_enkf_sliceJobs[slice_row,names(coef(slice_sim))])
  stew(file=slice_enkf_files[slice_row],seed=4000817+slice_row,{
    theta <- theta_eval
    # an extra step due to bug identified in enkf()
    sim_tmp <- slice_sim
    coef(sim_tmp) <- theta
    system.time(
      enkf(sim_tmp,
        Np=slice_enkf_Np
      ) -> slice_enkf_out
    ) -> slice_enkf_time
    enkf_logLik <- slice_enkf_out@loglik
    rm(slice_enkf_out)
    rm(sim_tmp)
  })
  c(logLik=enkf_logLik,theta, Np=slice_enkf_Np,
    U=length(unit_names(slice_sim)), N=length(time(slice_sim)),
    time=slice_enkf_time["elapsed"]
  )
} -> slice_enkf_results    
@

<<slice_mcap,eval=T,echo=F>>=

slice_abf_mcap <- mcap(
  logLik=slice_abf_results[,"logLik"],
  parameter=slice_abf_results[,"g"],
  span = 1
)

slice_ubf_mcap <- mcap(
  logLik=slice_ubf_results[,"logLik"],
  parameter=slice_ubf_results[,"g"],
  span = 0.8
)

slice_bpf_mcap <- mcap(
  logLik=slice_bpf_results[,"logLik"],
  parameter=slice_bpf_results[,"g"],
  span = 1
)

slice_enkf_mcap <- mcap(
  logLik=slice_enkf_results[,"logLik"],
  parameter=slice_enkf_results[,"g"],
  span = 1
)

  
plot.profile2 <- function(mcap1,ylab,xline=2.5,yline=2.5,xlab="",quadratic=FALSE,...){
  if(missing(ylab)) ylab <- "log likelihood"
  # par(mai=c(0.7,0.7,0.3,0.3))
  ggplot() + geom_point(aes(x = mcap1$parameter, y = mcap1$logLik)) + 
    geom_line(mapping = aes(x = mcap1$fit$parameter, y = mcap1$fit$smoothed),
              color = "red") +
    {if(quadratic) geom_line(mapping = aes(x = mcap1$fit$parameter, y = mcap1$fit$quadratic),
                             color = "blue",
                             lwd = 1.25)} +
    labs(x = xlab, y = ylab) +
    geom_vline(xintercept = mcap1$ci, color = "red") + 
    geom_hline(yintercept = max(mcap1$fit$smoothed, na.rm = T) - mcap1$delta, color = "red") +
    theme(panel.border = element_rect(colour = "black", fill=NA))
  }

@

<<slice_abf_plot, eval = F, echo=F,  fig.height=4, fig.width=5.5, out.width="4.5in", fig.cap = paste('Likelihood slice varying the coupling parameter, computed via {\\ABF} with $U=40$ cities. The solid lines construct a 95\\% Monte Carlo adjusted confidence interval \\citep{ionides17}. For this model realization, this interval includes the true parameter value identified by a blue dashed line.')>>=

if(0){
slice_results <- as.data.frame(rbind(
  slice_abf_results[,c("logLik","g")],
  slice_bpf_results[,c("logLik","g")]
))
slice_results$Method <- rep( c("ABF", "BPF"),each=nrow(slice_sliceJobs))
} else {
  slice_results <-  as.data.frame(slice_abf_results[,c("logLik","g")])
  slice_results$Method <- rep("ABF",each=nrow(slice_sliceJobs))
}

plot.profile2(slice_abf_mcap,xlab="G", quadratic = FALSE) +
  geom_vline(xintercept=measles_params["g"],color="blue",linetype="dashed")

@

<<slice_bpf_plot, eval = F, echo=F,  fig.height=4, fig.width=5.5, out.width="4.5in", fig.cap = paste('Likelihood slice varying the coupling parameter, computed via {BPF} with $U=40$ cities. The solid lines construct a 95\\% Monte Carlo adjusted confidence interval \\citep{ionides17}. For this model realization, this interval includes the true parameter value identified by a blue dashed line.')>>=


plot.profile2(slice_bpf_mcap,xlab="G", quadratic = FALSE) +
  geom_vline(xintercept=measles_params["g"],color="blue",linetype="dashed")

@


<<slice_enkf_plot, eval = F, echo=F,  fig.height=4, fig.width=5.5, out.width="4.5in", fig.cap = paste('Likelihood slice varying the coupling parameter, computed via {EnKF} with $U=40$ cities. The solid lines construct a 95\\% Monte Carlo adjusted confidence interval \\citep{ionides17}. For this model realization, this interval includes the true parameter value identified by a blue dashed line.')>>=


plot.profile2(slice_enkf_mcap,xlab="G", quadratic = FALSE) +
  geom_vline(xintercept=measles_params["g"],color="blue",linetype="dashed")

@



<<slice_ubf_plot, eval = F, echo=F,  fig.height=4, fig.width=5.5, out.width="4.5in", fig.cap = paste('Likelihood slice varying the coupling parameter, computed via {\\UBF} with $U=40$ cities. The solid lines construct a 95\\% Monte Carlo adjusted confidence interval \\citep{ionides17}. For this model realization, this interval includes the true parameter value identified by a blue dashed line.')>>=


plot.profile2(slice_ubf_mcap,xlab="G", quadratic = FALSE) +
  geom_vline(xintercept=measles_params["g"],color="blue",linetype="dashed")

@


<<slice_combined_plot, echo=F, fig.height=6, fig.width=4, out.height="7in", fig.cap = paste('Likelihood slices varying the coupling parameter, for the measles model with $U=40$ cities, computed via (A) {ABF}; (B) BPF; (C) EnKF. The solid perpendicular lines construct 95\\% Monte Carlo adjusted confidence intervals \\citep{ionides17}. The true parameter value is identified by a blue dashed line.')>>=

plot.profile <- function(mcap1,ylab,xline=2.5,yline=2.5,xlab="",...){
  if(missing(ylab)) ylab <- "log likelihood"
  plot(mcap1$logLik ~ mcap1$parameter, xlab="",ylab="",xaxt="n",...)
  mtext(side=2,text=ylab,line=yline)
  mtext(side=1,text=xlab,line=yline)
  lines(mcap1$fit$parameter,mcap1$fit$smoothed, col = "red", lwd = 1.5)
  abline(v=mcap1$ci,col="red")
  abline(h=max(mcap1$fit$smoothed,na.rm=T)-mcap1$delta,col="red")
}

par(omi=c(0.5,0.1,0.1,0.1))
par(mai=c(0.1,0.7,0.1,0.1))
par(mfrow=c(3,1))

plot.profile(slice_abf_mcap,xlab=""
# ,ylab="log likelihood (ABF)"
)
axis(side=1,labels=F)
abline(v=measles_params["g"],col="blue",lty="dashed")

yloc <- 0.05; xloc <- 570
mtext("A",side=2,line=2.5,las=1,padj=-4,cex=1.8)
text(xloc,
  (1-yloc)*min(slice_abf_mcap$logLik) + yloc*max(slice_abf_mcap$logLik),
  cex=1.4,"ABF")

plot.profile(slice_bpf_mcap,xlab="")
axis(side=1,labels=F)
abline(v=measles_params["g"],col="blue",lty="dashed")

mtext("B",side=2,line=2.5,las=1,padj=-4,cex=1.8)
text(xloc,
  (1-yloc)*min(slice_bpf_mcap$logLik) + yloc*max(slice_bpf_mcap$logLik),
  cex=1.4,"BPF")


plot.profile(slice_enkf_mcap,xlab="")
axis(side=1,labels=T)
abline(v=measles_params["g"],col="blue",lty="dashed")

mtext("C",side=2,line=2.5,las=1,padj=-4,cex=1.8)
mtext("G",side=1,line=2.5)
text(xloc,
  (1-yloc)*min(slice_enkf_mcap$logLik) + yloc*max(slice_enkf_mcap$logLik),
  cex=1.4,"EnKF")

@


Fig.~\ref{fig:slice_combined_plot}(A) demonstrates an application of {\ABF} to the task of computing a slice of the likelihood function over the coupling parameter, $\gravity$, for simulated data.
This slice varies $\gravity$ while fixing the other parameters at the values used for the simulation.
Fig.~\ref{fig:slice_combined_plot}(B) shows a similar plot calculated using BPF with comparable computational effort.
Both ABF and BPF are successful here, though BPF is more computationally efficient.
By contrast, Fig.~\ref{fig:slice_combined_plot}(C) shows that EnKF has substantial bias in estimating $\gravity$, as well as considerably lower likelihood. 
Likelihood slices have less inferential value than likelihood profiles, but provide a computationally and conceptually simpler setting that can be insightful.
Scientifically, the slices in Fig.~\ref{fig:slice_combined_plot} give an upper bound on the identifiability of $\gravity$ from such data, since the likelihood slice provides statistically efficient inference when all other parameters are known.

%%% dddddddddddddd

\subsection{Likelihood maximization and profile likelihood}
\label{sec:profile}
<<profile_plot, echo=F, fig.height=3, fig.width=5, out.width="4in", fig.cap = 'An iterated bagged filter used to maximize the likelihood, compute a profile likelihood, and hence construct a confidence interval. The profiling is carried out over the coupling parameter, $G$.'>>=

# see ./measles_iubf/README for how these .rda files were generated
iubf_dir <- './measles_iubf/'
rdanames <- system(paste('ls ', iubf_dir, 'generated/abf_outs_3/*.rda', sep = ''), intern = TRUE) # run level 3
g_prof <- data.frame()
for(fn in rdanames){
  load(fn)
  g_prof_row <- c('g' = final_params['g'],
                  'loglik' = ll[1],
                  'loglik_se' = ll[2])
  g_prof <- rbind(g_prof, g_prof_row)
}
colnames(g_prof) <- c('g', 'loglik', 'loglik_se')

plot.profile <- function(mcap1,ylab,xline=2.5,yline=2.5,xlab="",quadratic=FALSE,...){
   if(missing(ylab)) ylab <- "profile log likelihood"
   ggplot() + geom_point(aes(x = mcap1$parameter, y = mcap1$logLik), shape = 1) +
      geom_line(mapping = aes(x = mcap1$fit$parameter, y = mcap1$fit$smoothed),
                color = "red") +
      {if(quadratic) geom_line(mapping = aes(x = mcap1$fit$parameter, y = mcap1$fit$quadratic),
                               color = "blue",
                               lwd = 1.25)} +
      labs(x = xlab, y = ylab) +
      geom_vline(xintercept = mcap1$ci, color = "red") +
      geom_hline(yintercept = max(mcap1$fit$smoothed, na.rm = T) - mcap1$delta, color = "red") +
      theme(panel.border = element_rect(colour = "black", fill=NA))
}
g_prof %>% dplyr::group_by(g) %>% dplyr::filter(loglik==min(loglik)) -> mins
g_prof[which(!(g_prof$loglik %in% mins$loglik)),] -> top_g_prof

g_prof_mcap <- pomp::mcap(
   logLik=top_g_prof[,"loglik"],
   parameter=top_g_prof[,"g"],
   span = 0.90
)
plot.profile(g_prof_mcap,xlab='G', quadratic = FALSE) +
   geom_vline(xintercept=400,color="blue",linetype="dashed") +
   theme(axis.text.x = element_text(size = 12),
         axis.text.y = element_text(size = 12),
         axis.title.x = element_text(size = 14),
         axis.title.y = element_text(size = 14),
         panel.grid.major = element_blank(),
         panel.grid.minor = element_blank(),
         panel.background = element_blank(),
         panel.border = element_rect(colour = "black", fill=NA, size=0.8),
         axis.line = element_line(colour = "black"))


@


Likelihood evaluation via filtering does not by itself enable parameter estimation for POMP models, however it provides a foundation for Bayesian and likelihood-based inference.
In particular, filtering algorithms can be modified to carry out likelihood maximization by stochastically perturbing parameters in a sequence of filtering operations with decreasing perturbation variance \citep{ionides15}.
We demonstrate this for the measles model in Fig.~\ref{fig:profile_plot} using an iterated bagged filter algorithm which is fully described in Sec.~{\SuppSecParameterEst}.

Monte Carlo methods for computing and maximizing the log likelihood suffer from  bias and variance, both of which can be considerable for large datasets and complex models.
Appropriate inference methodology, such as Monte Carlo adjusted profile (MCAP) confidence intervals, can accommodate substantial Monte Carlo variance so long as the bias is slowly varying across the statistically plausible region of the parameter space \citep{ionides17,ning21}.
Fig.~\ref{fig:profile_plot} constructs an MCAP 95\% confidence interval for the coupling parameter, $G$, using an iterated unadapted bagged filter to maximize over the parameters, $a$, $\bar\beta$, $\sigma_{SE}$, $\psi$, $\mu_{EI}$ and $\mu_{IR}$.
This simulation study, carried out with $\Unit=20$ and $\Time=208$, shows that $G$ is identifiable via likelihood-based inference in the absence of assumptions about these parameters.

The likelihood estimate provided by bagged filters could be viewed as a composite likelihood \citep{varin11} rather than an approximation to the likelihood.
However, in situations where the likelihood approximation is found to be adequate, it is convenient to take advantage of the tools of likelihood-based inference. 

\section{Discussion}
\label{sec:discussion}

The nested loops used in the pseudocode for the bagged filters can be computed in various different orders to give mathematically equivalent results.
There is scope for implementations to trade off memory, computation and communication by varying decisions on how the loops defined in the pseudocode are coded, including decisions on memory over-writing and parallelization.
This article focuses on  the properties of the quantities calculated by the algorithms, leaving room for future research on implementation-specific considerations, though some supplementary discussion of memory-efficient implementation is given in Sec.~\SuppSecMemoryEfficient.

Plug-and-play inference based on sequential Monte Carlo likelihood evaluation has proved successful for investigating highly nonlinear partially observed dynamic systems of low dimension arising in analysis of epidemiological and ecological population dynamics \citep{breto18statSci,pons-salort18,decelles18,marino18}.
This article develops a methodological extension motivated by the analysis of interacting biological populations.
Similar challenges related to nonlinear non-Gaussian dynamic models arise in geophysical modeling.
Relative to biological systems, geophysical applications are characterized by a greater number of spatial locations, better mathematical understanding of the underlying processes, and lower stochasticity.
From this literature, the locally weighted particle filter of \citet{poterjoy16,poterjoy19} is perhaps closest to our approach, but the local weights of \citet{poterjoy16,poterjoy19} are used to construct a localized Kalman gain which is motivated by a Gaussian approximation comparable to EnKF. 
EnKF arose originally via geophysical research \citep{evensen94} and has since become used more widely for inference on SpatPOMP models \citep{katzfuss19,li20}.
However, EnKF can fail entirely even on simple POMP models if the structure is sufficiently non-Gaussian.
For example, let $X_n$ be a one-dimensional Gaussian random walk, and let $Y_n$ given $X_n=x_n$ be normally distributed with mean $0$ and variance $x_n^2$.
The linear filter rule used by EnKF to update the estimate of $X_n$ given $Y_n$ has mean zero for any value of $X_n$, since $X_n$ and $Y_n$ are uncorrelated.
Therefore, the EnKF filter estimate of the latent process remains essentially constant regardless of the data.
Models of this form are used in finance to describe stochastic volatility.
EnKF could be applied more successfully by modifying model, such as replacing $Y_n$ by $|Y_n|$, but for complex models it may be unclear whether and where such problems are arising.
Our results show that there is room for improvement over EnKF on a spatiotemporal epidemiology model, though in our example there is no clear advantage for BF methods over BPF.

Latent state trajectories constructed in our BF algorithms are all generated from the model simulator, appropriately reweighted and resampled, and so they are necessarily valid sample paths of the model.
For example, spatial smoothness properties of the model through space, or conservation properties where some function of the system remains unchanged through time, are maintained in the BF trajectories.
This is not true for the block particle filter, due to the indepdent resampling of the blocks (see Sec.~\SuppSecConstraint).
EnKF preserves linear constraints, since the filter procedure perturbs particles using a linear update rule, but cannot respect nonlinear relationships.
The practical importance of smoothness and conservation considerations will vary with the system under investigation, but this property of BF gives the scientific investigator one less thing to worry about. 


The algorithms {\UBF}, {\ABF}, {\ABFIR}, GIRF, PF, BPF, and EnKF compared in this article all enjoy the plug-and-play property, facilitating their implementations in general-purpose software.
The numerical results for this paper use the \code{abf}, \code{abfir}, \code{girf}, \code{pfilter}, \code{bpfilter} and \code{enkf} functions via the open-source R package \code{spatPomp} \citep{asfaw21arxiv} that provides a spatiotemporal extension of the R package \code{pomp} \citep{king16}.
{\UBF} was implemented using \code{abf} with $\Np=1$ particles per replicate.
The source code for this article is available as supplementary material.

\bibliography{bib-iif}

\end{document}
